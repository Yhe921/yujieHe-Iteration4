{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "import math\n",
    "from scipy.stats import nbinom\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "# Define the NB class first, not mixture version\n",
    "class NBNorm(nn.Module):\n",
    "    def __init__(self, c_in, c_out):\n",
    "        super(NBNorm,self).__init__()\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.n_conv = nn.Conv2d(in_channels=c_in,\n",
    "                                    out_channels=c_out,\n",
    "                                    kernel_size=(1,1),\n",
    "                                    bias=True)\n",
    "        \n",
    "        self.p_conv = nn.Conv2d(in_channels=c_in,\n",
    "                                    out_channels=c_out,\n",
    "                                    kernel_size=(1,1),\n",
    "                                    bias=True)\n",
    "        self.out_dim = c_out # output horizon\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x.permute(0,2,1,3)\n",
    "        (B, _, N,_) = x.shape # B: batch_size; N: input nodes\n",
    "        n = self.n_conv(x).squeeze_(-1)\n",
    "        p = self.p_conv(x).squeeze_(-1)\n",
    "\n",
    "        # Reshape\n",
    "        n = n.view([B,self.out_dim,N])\n",
    "        p = p.view([B,self.out_dim,N])\n",
    "\n",
    "        # Ensure n is positive and p between 0 and 1\n",
    "        n = F.softplus(n) # Some parameters can be tuned here\n",
    "        p = F.sigmoid(p)\n",
    "        return n.permute([0,2,1]), p.permute([0,2,1])\n",
    "\n",
    "    def likelihood_loss(self,y,n,p,y_mask=None):\n",
    "        \"\"\"\n",
    "        y: true values\n",
    "        y_mask: whether missing mask is given\n",
    "        \"\"\"\n",
    "        nll = torch.lgamma(n) + torch.lgamma(y+1) - torch.lgamma(n+y) - n*torch.log(p) - y*torch.log(1-p)\n",
    "        if y_mask is not None:\n",
    "            nll = nll*y_mask\n",
    "        return torch.sum(nll)\n",
    "\n",
    "    def mean(self,n,p):\n",
    "        \"\"\"\n",
    "        :param cat: Input data of shape (batch_size, num_timesteps, in_nodes)\n",
    "        :return: Output data of shape (batch_size, 1, num_timesteps, in_nodes)\n",
    "        \"\"\" \n",
    "        pass\n",
    "\n",
    "# Define the Gaussian \n",
    "class GaussNorm(nn.Module):\n",
    "    def __init__(self, c_in, c_out):\n",
    "        super(GaussNorm,self).__init__()\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.n_conv = nn.Conv2d(in_channels=c_in,\n",
    "                                    out_channels=c_out,\n",
    "                                    kernel_size=(1,1),\n",
    "                                    bias=True)\n",
    "        \n",
    "        self.p_conv = nn.Conv2d(in_channels=c_in,\n",
    "                                    out_channels=c_out,\n",
    "                                    kernel_size=(1,1),\n",
    "                                    bias=True)\n",
    "        self.out_dim = c_out # output horizon\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x.permute(0,2,1,3)\n",
    "        (B, _, N,_) = x.shape # B: batch_size; N: input nodes\n",
    "        loc    = self.n_conv(x).squeeze_(-1) # The location (loc) keyword specifies the mean. The scale (scale) keyword specifies the standard deviation.\n",
    "        scale  = self.p_conv(x).squeeze_(-1)\n",
    "\n",
    "        # Reshape\n",
    "        loc   = loc.view([B,self.out_dim,N])\n",
    "        scale = scale.view([B,self.out_dim,N])\n",
    "\n",
    "        # Ensure n is positive and p between 0 and 1\n",
    "        loc = F.softplus(loc) # Some parameters can be tuned here, count data are always positive\n",
    "        # loc = F.sigmoid(loc) # Some parameters can be tuned here, count data are always positive\n",
    "        scale = F.sigmoid(scale)\n",
    "        return loc.permute([0,2,1]), scale.permute([0,2,1])\n",
    "\n",
    "# Define the NB class first, not mixture version\n",
    "class NBNorm_ZeroInflated(nn.Module):\n",
    "    def __init__(self, c_in, c_out, four=False):\n",
    "        super(NBNorm_ZeroInflated,self).__init__()\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.n_conv = nn.Conv2d(in_channels=c_in,\n",
    "                                    out_channels=c_out,\n",
    "                                    kernel_size=(1,1),\n",
    "                                    bias=True)\n",
    "        \n",
    "        self.p_conv = nn.Conv2d(in_channels=c_in,\n",
    "                                    out_channels=c_out,\n",
    "                                    kernel_size=(1,1),\n",
    "                                    bias=True)\n",
    "\n",
    "        self.pi_conv = nn.Conv2d(in_channels=c_in,\n",
    "                                    out_channels=c_out,\n",
    "                                    kernel_size=(1,1),\n",
    "                                    bias=True)\n",
    "        self.four = four\n",
    "\n",
    "        if four:\n",
    "            self.zero_conv = nn.Conv2d(in_channels=c_in,\n",
    "                                        out_channels=c_out,\n",
    "                                        kernel_size=(1,1),\n",
    "                                        bias=True)\n",
    "\n",
    "        self.out_dim = c_out # output horizon\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x.permute(0,2,1,3)\n",
    "        (B, _, N,_) = x.shape # B: batch_size; N: input nodes\n",
    "        n  = self.n_conv(x).squeeze_(-1)\n",
    "        p  = self.p_conv(x).squeeze_(-1)\n",
    "        pi = self.pi_conv(x).squeeze_(-1)\n",
    "\n",
    "        # Reshape\n",
    "        n = n.view([B,self.out_dim,N])\n",
    "        p = p.view([B,self.out_dim,N])\n",
    "        pi = pi.view([B,self.out_dim,N])\n",
    "\n",
    "        if self.four:\n",
    "            zi = self.zero_conv(x).squeeze_(-1)\n",
    "            zi = zi.view([B,self.out_dim,N])\n",
    "            zi = F.sigmoid(zi)\n",
    "\n",
    "        # Ensure n is positive and p between 0 and 1\n",
    "        if not self.four:\n",
    "            n = F.softplus(n)  # Some parameters can be tuned here     # fixme\n",
    "            p = F.sigmoid(p)\n",
    "            pi = F.sigmoid(pi)      # todo\n",
    "        if self.four:\n",
    "            return n.permute([0,2,1]), p.permute([0,2,1]), pi.permute([0,2,1]), zi.permute([0,2,1])\n",
    "        else:\n",
    "            return n.permute([0,2,1]), p.permute([0,2,1]), pi.permute([0,2,1])\n",
    "\n",
    "class D_GCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network block that applies a diffusion graph convolution to sampled location\n",
    "    \"\"\"       \n",
    "    def __init__(self, in_channels, out_channels, orders, activation = 'relu', att=False):\n",
    "        \"\"\"\n",
    "        :param in_channels: Number of time step.\n",
    "        :param out_channels: Desired number of output features at each node in\n",
    "        each time step.\n",
    "        :param order: The diffusion steps.\n",
    "        \"\"\"\n",
    "        super(D_GCN, self).__init__()\n",
    "        self.orders = orders\n",
    "        self.activation = activation\n",
    "        self.num_matrices = 2 * self.orders + 1\n",
    "        self.Theta1 = nn.Parameter(torch.FloatTensor(in_channels * self.num_matrices,\n",
    "                                             out_channels))\n",
    "        self.bias = nn.Parameter(torch.FloatTensor(out_channels))\n",
    "        self.att = att\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.Theta1.shape[1])\n",
    "        self.Theta1.data.uniform_(-stdv, stdv)\n",
    "        stdv1 = 1. / math.sqrt(self.bias.shape[0])\n",
    "        self.bias.data.uniform_(-stdv1, stdv1)\n",
    "        \n",
    "    def _concat(self, x, x_):\n",
    "        x_ = x_.unsqueeze(0)\n",
    "        return torch.cat([x, x_], dim=0)\n",
    "        \n",
    "    def forward(self, X, A_q, A_h):\n",
    "        \"\"\"\n",
    "        :param X: Input data of shape (batch_size, num_nodes, num_timesteps)\n",
    "        :A_q: The forward random walk matrix (num_nodes, num_nodes)\n",
    "        :A_h: The backward random walk matrix (num_nodes, num_nodes)\n",
    "        :return: Output data of shape (batch_size, num_nodes, num_features)\n",
    "        \"\"\"\n",
    "        batch_size = X.shape[0] # batch_size\n",
    "        num_node = X.shape[1]\n",
    "        input_size = X.size(2)  # time_length\n",
    "        supports = []\n",
    "        supports.append(A_q)\n",
    "        supports.append(A_h)\n",
    "        \n",
    "        x0 = X.permute(1, 2, 0) #(num_nodes, num_times, batch_size)\n",
    "        x0 = torch.reshape(x0, shape=[num_node, input_size * batch_size])\n",
    "        x = torch.unsqueeze(x0, 0)\n",
    "        for support in supports:\n",
    "            x1 = torch.mm(support, x0)\n",
    "            x = self._concat(x, x1)\n",
    "            for k in range(2, self.orders + 1):\n",
    "                x2 = 2 * torch.mm(support, x1) - x0\n",
    "                x = self._concat(x, x2)\n",
    "                x1, x0 = x2, x1\n",
    "                \n",
    "        x = torch.reshape(x, shape=[self.num_matrices, num_node, input_size, batch_size])\n",
    "        x = x.permute(3, 1, 2, 0)  # (batch_size, num_nodes, input_size, order)\n",
    "        x = torch.reshape(x, shape=[batch_size, num_node, input_size * self.num_matrices])         \n",
    "        x = torch.matmul(x, self.Theta1)   # (batch_size * self._num_nodes, output_size)\n",
    "\n",
    "        if self.att:\n",
    "            att = torch.softmax(torch.tanh(x), dim=1)       # attention layer\n",
    "            x = x * att\n",
    "\n",
    "        x += self.bias\n",
    "        if self.activation == 'relu':\n",
    "            x = F.relu(x)\n",
    "        elif self.activation == 'selu':\n",
    "            x = F.selu(x)   \n",
    "            \n",
    "        return x\n",
    "\n",
    "## Code of BTCN from Yuankai\n",
    "class B_TCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network block that applies a bidirectional temporal convolution to each node of\n",
    "    a graph.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3,activation = 'relu',device='cuda:0'):\n",
    "        \"\"\"\n",
    "        :param in_channels: Number of nodes in the graph.\n",
    "        :param out_channels: Desired number of output features.\n",
    "        :param kernel_size: Size of the 1D temporal kernel.\n",
    "        \"\"\"\n",
    "        super(B_TCN, self).__init__()\n",
    "        # forward dirction temporal convolution\n",
    "        self.kernel_size = kernel_size\n",
    "        self.out_channels = out_channels\n",
    "        self.activation = activation\n",
    "        self.device = device\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, (1, kernel_size))\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, (1, kernel_size))\n",
    "        self.conv3 = nn.Conv2d(in_channels, out_channels, (1, kernel_size))\n",
    "        \n",
    "        self.conv1b = nn.Conv2d(in_channels, out_channels, (1, kernel_size))\n",
    "        self.conv2b = nn.Conv2d(in_channels, out_channels, (1, kernel_size))\n",
    "        self.conv3b = nn.Conv2d(in_channels, out_channels, (1, kernel_size))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        :param X: Input data of shape (batch_size, num_timesteps, num_nodes)\n",
    "        :return: Output data of shape (batch_size, num_timesteps, num_features)\n",
    "        \"\"\"\n",
    "        batch_size = X.shape[0]\n",
    "        seq_len = X.shape[1]\n",
    "        Xf = X.unsqueeze(1)  # (batch_size, 1, num_timesteps, num_nodes)\n",
    "        \n",
    "        inv_idx = torch.arange(Xf.size(2)-1, -1, -1).long().to(device=self.device)#.to(device=self.device).to(device=self.device)\n",
    "        Xb = Xf.index_select(2, inv_idx) # inverse the direction of time\n",
    "        \n",
    "        Xf = Xf.permute(0, 3, 1, 2)\n",
    "        Xb = Xb.permute(0, 3, 1, 2) #(batch_size, num_nodes, 1, num_timesteps)\n",
    "        tempf = self.conv1(Xf) * torch.sigmoid(self.conv2(Xf)) #+\n",
    "        outf = tempf + self.conv3(Xf) \n",
    "        outf = outf.reshape([batch_size, seq_len - self.kernel_size + 1, self.out_channels])        \n",
    "        \n",
    "        tempb = self.conv1b(Xb) * torch.sigmoid(self.conv2b(Xb)) #+\n",
    "        outb = tempb + self.conv3b(Xb)\n",
    "        outb = outb.reshape([batch_size, seq_len - self.kernel_size + 1, self.out_channels])\n",
    "        \n",
    "        rec = torch.zeros([batch_size, self.kernel_size - 1, self.out_channels]).to(device=self.device)#.to(device=self.device)\n",
    "        outf = torch.cat((outf, rec), dim = 1)\n",
    "        outb = torch.cat((outb, rec), dim = 1) #(batch_size, num_timesteps, out_features)\n",
    "        \n",
    "        inv_idx = torch.arange(outb.size(1)-1, -1, -1).long().to(device=self.device)#.to(device=self.device)\n",
    "        outb = outb.index_select(1, inv_idx)\n",
    "        out = outf + outb\n",
    "        if self.activation == 'relu':\n",
    "            out = F.relu(outf) + F.relu(outb)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            out = F.sigmoid(outf) + F.sigmoid(outb)       \n",
    "        return out\n",
    "\n",
    "\n",
    "class ST_NB(nn.Module):\n",
    "    \"\"\"\n",
    "  wx_t  + wx_s\n",
    "    |       |\n",
    "   TC4     SC4\n",
    "    |       |\n",
    "   TC3     SC3\n",
    "    |       |\n",
    "   z_t     z_s\n",
    "    |       |\n",
    "   TC2     SC2\n",
    "    |       |  \n",
    "   TC1     SC1\n",
    "    |       |\n",
    "   x_m     x_m\n",
    "    \"\"\"\n",
    "    def __init__(self, SC1, SC2, SC3, TC1, TC2, TC3, SNB,TNB): \n",
    "        super(ST_NB, self).__init__()\n",
    "        self.TC1 = TC1\n",
    "        self.TC2 = TC2\n",
    "        self.TC3 = TC3\n",
    "        self.TNB = TNB\n",
    "\n",
    "        self.SC1 = SC1\n",
    "        self.SC2 = SC2\n",
    "        self.SC3 = SC3\n",
    "        self.SNB = SNB\n",
    "\n",
    "    def forward(self, X, A_q, A_h):\n",
    "        \"\"\"\n",
    "        :param X: Input data of shape (batch_size, num_timesteps, num_nodes)\n",
    "        :A_hat: The Laplacian matrix (num_nodes, num_nodes)\n",
    "        :return: Reconstructed X of shape (batch_size, num_timesteps, num_nodes)\n",
    "        \"\"\"\n",
    "        print(111)\n",
    "        print(111)\n",
    "        X = X[:,:,:,0] # Dummy dimension deleted\n",
    "        X_T = X.permute(0,2,1)\n",
    "        X_t1 = self.TC1(X_T)\n",
    "        X_t2 = self.TC2(X_t1) #num_time, rank\n",
    "        self.temporal_factors = X_t2\n",
    "        X_t3 = self.TC3(X_t2)\n",
    "        _b,_h,_ht = X_t3.shape\n",
    "        n_t_nb,p_t_nb = self.TNB(X_t3.view(_b,_h,_ht,1))\n",
    "\n",
    "        X_s1 = self.SC1(X, A_q, A_h)\n",
    "        X_s2 = self.SC2(X_s1, A_q, A_h) #num_nodes, rank\n",
    "        self.space_factors = X_s2\n",
    "        X_s3 = self.SC3(X_s2, A_q, A_h)\n",
    "        _b,_n,_hs = X_s3.shape\n",
    "        n_s_nb,p_s_nb = self.SNB(X_s3.view(_b,_n,_hs,1))\n",
    "        n_res = n_t_nb.permute(0, 2, 1) * n_s_nb\n",
    "        p_res = p_t_nb.permute(0, 2, 1) * p_s_nb\n",
    "               \n",
    "        return n_res,p_res\n",
    "\n",
    "class ST_Gau(nn.Module):\n",
    "    \"\"\"\n",
    "  wx_t  + wx_s\n",
    "    |       |\n",
    "   TC4     SC4\n",
    "    |       |\n",
    "   TC3     SC3\n",
    "    |       |\n",
    "   z_t     z_s\n",
    "    |       |\n",
    "   TC2     SC2\n",
    "    |       |  \n",
    "   TC1     SC1\n",
    "    |       |\n",
    "   x_m     x_m\n",
    "    \"\"\"\n",
    "    def __init__(self, SC1, SC2, SC3, TC1, TC2, TC3, SGau,TGau): \n",
    "        super(ST_Gau, self).__init__()\n",
    "        self.TC1 = TC1\n",
    "        self.TC2 = TC2\n",
    "        self.TC3 = TC3\n",
    "        self.TGau = TGau\n",
    "\n",
    "        self.SC1 = SC1\n",
    "        self.SC2 = SC2\n",
    "        self.SC3 = SC3\n",
    "        self.SGau = SGau\n",
    "\n",
    "    def forward(self, X, A_q, A_h):\n",
    "        \"\"\"\n",
    "        :param X: Input data of shape (batch_size, num_timesteps, num_nodes)\n",
    "        :A_hat: The Laplacian matrix (num_nodes, num_nodes)\n",
    "        :return: Reconstructed X of shape (batch_size, num_timesteps, num_nodes)\n",
    "        \"\"\"  \n",
    "        X = X[:,:,:,0] #.to(device='cuda') # Dummy dimension deleted\n",
    "        X_T = X.permute(0,2,1)\n",
    "        X_t1 = self.TC1(X_T)\n",
    "        X_t2 = self.TC2(X_t1) #num_time, rank\n",
    "        self.temporal_factors = X_t2\n",
    "        X_t3 = self.TC3(X_t2)\n",
    "        _b,_h,_ht = X_t3.shape\n",
    "        loc_t,scale_t = self.TGau(X_t3.view(_b,_h,_ht,1))\n",
    "\n",
    "        X_s1 = self.SC1(X, A_q, A_h)\n",
    "        X_s2 = self.SC2(X_s1, A_q, A_h) #num_nodes, rank\n",
    "        self.space_factors = X_s2\n",
    "        X_s3 = self.SC3(X_s2, A_q, A_h)\n",
    "        _b,_n,_hs = X_s3.shape\n",
    "        loc_s,scale_s = self.SGau(X_s3.view(_b,_n,_hs,1))\n",
    "\n",
    "        loc_res = loc_t.permute(0, 2, 1) * loc_s\n",
    "        scale_res = scale_t.permute(0, 2, 1) * scale_s\n",
    "               \n",
    "        return loc_res,scale_res\n",
    "\n",
    "class ST_NB_ZeroInflated(nn.Module):\n",
    "    \"\"\"\n",
    "  wx_t  + wx_s\n",
    "    |       |\n",
    "   TC4     SC4\n",
    "    |       |\n",
    "   TC3     SC3\n",
    "    |       |\n",
    "   z_t     z_s\n",
    "    |       |\n",
    "   TC2     SC2\n",
    "    |       |  \n",
    "   TC1     SC1\n",
    "    |       |\n",
    "   x_m     x_m\n",
    "    \"\"\"\n",
    "    def __init__(self, SC1, SC2, SC3, TC1, TC2, TC3, SNB,TNB): \n",
    "        super(ST_NB_ZeroInflated, self).__init__()\n",
    "        self.TC1 = TC1\n",
    "        self.TC2 = TC2\n",
    "        self.TC3 = TC3\n",
    "        self.TNB = TNB\n",
    "\n",
    "        self.SC1 = SC1\n",
    "        self.SC2 = SC2\n",
    "        self.SC3 = SC3\n",
    "        self.SNB = SNB\n",
    "\n",
    "    def forward(self, X, A_q, A_h):\n",
    "        \"\"\"\n",
    "        :param X: Input data of shape (batch_size, num_timesteps, num_nodes)\n",
    "        :A_hat: The Laplacian matrix (num_nodes, num_nodes)\n",
    "        :return: Reconstructed X of shape (batch_size, num_timesteps, num_nodes)\n",
    "        \"\"\"  \n",
    "        X = X[:,:,:,0]#.to(device='cuda') # Dummy dimension deleted\n",
    "        X_T = X.permute(0,2,1)\n",
    "        X_t1 = self.TC1(X_T)\n",
    "        X_t2 = self.TC2(X_t1) #num_time, rank\n",
    "        self.temporal_factors = X_t2\n",
    "        X_t3 = self.TC3(X_t2)\n",
    "        _b,_h,_ht = X_t3.shape\n",
    "        n_t_nb,p_t_nb,pi_t_nb = self.TNB(X_t3.view(_b,_h,_ht,1))\n",
    "\n",
    "        X_s1 = self.SC1(X, A_q, A_h)\n",
    "        X_s2 = self.SC2(X_s1, A_q, A_h) #num_nodes, rank\n",
    "        self.space_factors = X_s2\n",
    "        X_s3 = self.SC3(X_s2, A_q, A_h)\n",
    "        _b,_n,_hs = X_s3.shape\n",
    "        n_s_nb,p_s_nb,pi_s_nb = self.SNB(X_s3.view(_b,_n,_hs,1))\n",
    "        n_res = n_t_nb.permute(0, 2, 1) * n_s_nb\n",
    "        p_res = p_t_nb.permute(0, 2, 1) * p_s_nb\n",
    "        pi_res = pi_t_nb.permute(0, 2, 1) * pi_s_nb\n",
    "\n",
    "        return n_res,p_res,pi_res\n",
    "\n",
    "\n",
    "class ST_TWEEDIE_ZeroInflated(nn.Module):\n",
    "    \"\"\"\n",
    "  wx_t  + wx_s\n",
    "    |       |\n",
    "   TC4     SC4\n",
    "    |       |\n",
    "   TC3     SC3\n",
    "    |       |\n",
    "   z_t     z_s\n",
    "    |       |\n",
    "   TC2     SC2\n",
    "    |       |\n",
    "   TC1     SC1\n",
    "    |       |\n",
    "   x_m     x_m\n",
    "    \"\"\"\n",
    "    def __init__(self, SC1, SC2, SC3, TC1, TC2, TC3, SNB,TNB):\n",
    "        super(ST_TWEEDIE_ZeroInflated, self).__init__()\n",
    "        self.TC1 = TC1\n",
    "        self.TC2 = TC2\n",
    "        self.TC3 = TC3\n",
    "        self.TNB = TNB\n",
    "\n",
    "        self.SC1 = SC1\n",
    "        self.SC2 = SC2\n",
    "        self.SC3 = SC3\n",
    "        self.SNB = SNB\n",
    "\n",
    "    def forward(self, X, A_q, A_h):\n",
    "        \"\"\"\n",
    "        :param X: Input data of shape (batch_size, num_timesteps, num_nodes)\n",
    "        :A_hat: The Laplacian matrix (num_nodes, num_nodes)\n",
    "        :return: Reconstructed X of shape (batch_size, num_timesteps, num_nodes)\n",
    "        \"\"\"\n",
    "        X = X[:,:,:,0]#.to(device='cuda') # Dummy dimension deleted\n",
    "        X_T = X.permute(0,2,1)\n",
    "        X_t1 = self.TC1(X_T)\n",
    "        X_t2 = self.TC2(X_t1) #num_time, rank\n",
    "        self.temporal_factors = X_t2\n",
    "        X_t3 = self.TC3(X_t2)\n",
    "        _b,_h,_ht = X_t3.shape\n",
    "        _, _,pi_t_nb = self.TNB(X_t3.view(_b,_h,_ht,1))\n",
    "\n",
    "        pi_t_nb = torch.sigmoid(pi_t_nb)\n",
    "\n",
    "        X_s1 = self.SC1(X, A_q, A_h)\n",
    "        X_s2 = self.SC2(X_s1, A_q, A_h) #num_nodes, rank\n",
    "        self.space_factors = X_s2\n",
    "        X_s3 = self.SC3(X_s2, A_q, A_h)\n",
    "        _b,_n,_hs = X_s3.shape\n",
    "        _,_,pi_s_nb = self.SNB(X_s3.view(_b,_n,_hs,1))\n",
    "\n",
    "        pi_s_nb = torch.sigmoid(pi_s_nb)\n",
    "\n",
    "        pi_res = pi_t_nb.permute(0, 2, 1) * pi_s_nb\n",
    "\n",
    "        return 0, 0, pi_res\n",
    "\n",
    "\n",
    "class ST_new_TWEEDIE_ZeroInflated(nn.Module):\n",
    "    \"\"\"\n",
    "  wx_t  + wx_s\n",
    "    |       |\n",
    "   TC4     SC4\n",
    "    |       |\n",
    "   TC3     SC3\n",
    "    |       |\n",
    "   z_t     z_s\n",
    "    |       |\n",
    "   TC2     SC2\n",
    "    |       |\n",
    "   TC1     SC1\n",
    "    |       |\n",
    "   x_m     x_m\n",
    "    \"\"\"\n",
    "    def __init__(self, SC1, SC2, SC3, TC1, TC2, TC3, SNB,TNB, four=False):\n",
    "        super(ST_new_TWEEDIE_ZeroInflated, self).__init__()\n",
    "        self.TC1 = TC1\n",
    "        self.TC2 = TC2\n",
    "        self.TC3 = TC3\n",
    "        self.TNB = TNB\n",
    "\n",
    "        self.SC1 = SC1\n",
    "        self.SC2 = SC2\n",
    "        self.SC3 = SC3\n",
    "        self.SNB = SNB\n",
    "        self.four = four\n",
    "\n",
    "    def forward(self, X, A_q, A_h):\n",
    "        \"\"\"\n",
    "        :param X: Input data of shape (batch_size, num_timesteps, num_nodes)\n",
    "        :A_hat: The Laplacian matrix (num_nodes, num_nodes)\n",
    "        :return: Reconstructed X of shape (batch_size, num_timesteps, num_nodes)\n",
    "        \"\"\"\n",
    "        X = X[:,:,:,0]#.to(device='cuda') # Dummy dimension deleted\n",
    "        X_T = X.permute(0,2,1)\n",
    "        X_t1 = self.TC1(X_T)\n",
    "        X_t2 = self.TC2(X_t1) #num_time, rank\n",
    "        self.temporal_factors = X_t2\n",
    "        X_t3 = self.TC3(X_t2)\n",
    "        _b,_h,_ht = X_t3.shape\n",
    "        if self.four:\n",
    "            n_t_nb, p_t_nb, pi_t_nb, zi_t_nb = self.TNB(X_t3.view(_b, _h, _ht, 1))\n",
    "        else:\n",
    "            n_t_nb,p_t_nb,pi_t_nb = self.TNB(X_t3.view(_b,_h,_ht,1))\n",
    "\n",
    "        X_s1 = self.SC1(X, A_q, A_h)\n",
    "        X_s2 = self.SC2(X_s1, A_q, A_h) #num_nodes, rank\n",
    "        self.space_factors = X_s2\n",
    "        X_s3 = self.SC3(X_s2, A_q, A_h)\n",
    "        _b,_n,_hs = X_s3.shape\n",
    "        if self.four:\n",
    "            n_s_nb, p_s_nb, pi_s_nb, zi_s_nb = self.SNB(X_s3.view(_b, _n, _hs, 1))\n",
    "\n",
    "            zi_res = zi_t_nb.permute(0, 2, 1) * zi_s_nb\n",
    "            # zi_res = torch.sigmoid(zi_res)\n",
    "        else:\n",
    "            n_s_nb,p_s_nb,pi_s_nb = self.SNB(X_s3.view(_b,_n,_hs,1))\n",
    "\n",
    "        phi_res = n_t_nb.permute(0, 2, 1) * n_s_nb\n",
    "        rou_res = p_t_nb.permute(0, 2, 1) * p_s_nb\n",
    "        mu_res = pi_t_nb.permute(0, 2, 1) * pi_s_nb\n",
    "\n",
    "        rou_res = torch.sigmoid(rou_res) + 1\n",
    "        phi_res = torch.relu(phi_res)\n",
    "\n",
    "        # n, p, pi, zi => phi, rou, mu, zi\n",
    "        if self.four:\n",
    "            return phi_res, rou_res, mu_res, zi_res\n",
    "        else:\n",
    "            return phi_res, rou_res, mu_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "# from sklearn.externals import joblib\n",
    "import joblib\n",
    "import scipy.io\n",
    "import torch\n",
    "from torch import nn\n",
    "from scipy.stats import nbinom,norm\n",
    "rand = np.random.RandomState(0)\n",
    "# import tweedie\n",
    "import torch.distributions\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "\n",
    "\"\"\"\n",
    "Geographical information calculation\n",
    "\"\"\"\n",
    "def get_long_lat(sensor_index,loc = None):\n",
    "    \"\"\"\n",
    "        Input the index out from 0-206 to access the longitude and latitude of the nodes\n",
    "    \"\"\"\n",
    "    if loc is None:\n",
    "        locations = pd.read_csv('data/metr/graph_sensor_locations.csv')\n",
    "    else:\n",
    "        locations = loc\n",
    "    lng = locations['longitude'].loc[sensor_index]\n",
    "    lat = locations['latitude'].loc[sensor_index]\n",
    "    return lng.to_numpy(),lat.to_numpy()\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2): \n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    " \n",
    "    # haversine\n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 \n",
    "    return c * r * 1000\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Generate the training sample for forecasting task, same idea from STGCN\n",
    "\"\"\"\n",
    "\n",
    "def generate_dataset(X, num_timesteps_input, num_timesteps_output, origional_feature=True):\n",
    "    \"\"\"\n",
    "    Takes node features for the graph and divides them into multiple samples\n",
    "    along the time-axis by sliding a window of size (num_timesteps_input+\n",
    "    num_timesteps_output) across it in steps of 1.\n",
    "    :param X: Node features of shape (num_vertices, num_features,\n",
    "    num_timesteps)\n",
    "    :return:\n",
    "        - Node features divided into multiple samples. Shape is\n",
    "          (num_samples, num_vertices, num_features, num_timesteps_input).\n",
    "        - Node targets for the samples. Shape is\n",
    "          (num_samples, num_vertices, num_features, num_timesteps_output).\n",
    "    \"\"\"\n",
    "    # Generate the beginning index and the ending index of a sample, which\n",
    "    # contains (num_points_for_training + num_points_for_predicting) points\n",
    "    indices = [(i, i + (num_timesteps_input + num_timesteps_output)) for i\n",
    "               in range(X.shape[2] - (\n",
    "                num_timesteps_input + num_timesteps_output) + 1)]\n",
    "\n",
    "    # Save samples\n",
    "    features, target = [], []\n",
    "    for i, j in indices:\n",
    "        features.append(\n",
    "            X[:, :, i: i + num_timesteps_input].transpose(\n",
    "                (0, 2, 1)))\n",
    "        target.append(X[:, 0, i + num_timesteps_input: j])\n",
    "\n",
    "    return torch.from_numpy(np.array(features)), \\\n",
    "           torch.from_numpy(np.array(target))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Dynamically construct the adjacent matrix\n",
    "\"\"\"\n",
    "\n",
    "def get_Laplace(A):\n",
    "    \"\"\"\n",
    "    Returns the laplacian adjacency matrix. This is for C_GCN\n",
    "    \"\"\"\n",
    "    if A[0, 0] == 1:\n",
    "        A = A - np.diag(np.ones(A.shape[0], dtype=np.float32)) # if the diag has been added by 1s\n",
    "    D = np.array(np.sum(A, axis=1)).reshape((-1,))\n",
    "    D[D <= 10e-5] = 10e-5\n",
    "    diag = np.reciprocal(np.sqrt(D))\n",
    "    A_wave = np.multiply(np.multiply(diag.reshape((-1, 1)), A),\n",
    "                         diag.reshape((1, -1)))\n",
    "    return A_wave\n",
    "\n",
    "def get_normalized_adj(A):\n",
    "    \"\"\"\n",
    "    Returns the degree normalized adjacency matrix. This is for K_GCN\n",
    "    \"\"\"\n",
    "    if A[0, 0] == 0:\n",
    "        A = A + np.diag(np.ones(A.shape[0], dtype=np.float32)) # if the diag has been added by 1s\n",
    "    D = np.array(np.sum(A, axis=1)).reshape((-1,))\n",
    "    D[D <= 10e-5] = 10e-5    # Prevent infs\n",
    "    diag = np.reciprocal(np.sqrt(D))\n",
    "    A_wave = np.multiply(np.multiply(diag.reshape((-1, 1)), A),\n",
    "                         diag.reshape((1, -1)))\n",
    "    return A_wave\n",
    "\n",
    "def calculate_random_walk_matrix(adj_mx):\n",
    "    \"\"\"\n",
    "    Returns the random walk adjacency matrix. This is for D_GCN\n",
    "    \"\"\"\n",
    "    adj_mx = sp.coo_matrix(adj_mx)\n",
    "    d = np.array(adj_mx.sum(1))\n",
    "    d_inv = np.power(d, -1).flatten()\n",
    "    d_inv[np.isinf(d_inv)] = 0.\n",
    "    d_mat_inv = sp.diags(d_inv)\n",
    "    random_walk_mx = d_mat_inv.dot(adj_mx).tocoo()\n",
    "    return random_walk_mx.toarray()\n",
    "\n",
    "\n",
    "def test_error_virtual(STmodel, unknow_set, test_data, A_s, E_maxvalue, Missing0):\n",
    "    \"\"\"\n",
    "    :param STmodel: The graph neural networks\n",
    "    :unknow_set: The unknow locations for spatial prediction\n",
    "    :test_data: The true value test_data of shape (test_num_timesteps, num_nodes)\n",
    "    :A_s: The full adjacent matrix\n",
    "    :Missing0: True: 0 in original datasets means missing data\n",
    "    :return: NAE, MAPE and RMSE\n",
    "    \"\"\"  \n",
    "    unknow_set = set(unknow_set)\n",
    "    time_dim = STmodel.time_dimension\n",
    "    \n",
    "    test_omask = np.ones(test_data.shape)\n",
    "    if Missing0 == True:\n",
    "        test_omask[test_data == 0] = 0\n",
    "    test_inputs = (test_data * test_omask).astype('float32')\n",
    "    test_inputs_s = test_inputs\n",
    "   \n",
    "    missing_index = np.ones(np.shape(test_data))\n",
    "    missing_index[:, list(unknow_set)] = 0\n",
    "    missing_index_s = missing_index\n",
    "    \n",
    "    o = np.zeros([test_data.shape[0]//time_dim*time_dim, test_inputs_s.shape[1]]) #Separate the test data into several h period\n",
    "    \n",
    "    for i in range(0, test_data.shape[0]//time_dim*time_dim, time_dim):\n",
    "        inputs = test_inputs_s[i:i+time_dim, :]\n",
    "        missing_inputs = missing_index_s[i:i+time_dim, :]\n",
    "        T_inputs = inputs*missing_inputs\n",
    "        T_inputs = T_inputs/E_maxvalue\n",
    "        T_inputs = np.expand_dims(T_inputs, axis = 0)\n",
    "        T_inputs = torch.from_numpy(T_inputs.astype('float32'))\n",
    "        A_q = torch.from_numpy((calculate_random_walk_matrix(A_s).T).astype('float32'))\n",
    "        A_h = torch.from_numpy((calculate_random_walk_matrix(A_s.T).T).astype('float32'))\n",
    "        \n",
    "        imputation = STmodel(T_inputs, A_q, A_h)\n",
    "        imputation = imputation.data.numpy()\n",
    "        o[i:i+time_dim, :] = imputation[0, :, :]\n",
    "    \n",
    "    o = o*E_maxvalue \n",
    "    truth = test_inputs_s[0:test_data.shape[0]//time_dim*time_dim]\n",
    "    o[missing_index_s[0:test_data.shape[0]//time_dim*time_dim] == 1] = truth[missing_index_s[0:test_data.shape[0]//time_dim*time_dim] == 1]\n",
    "    \n",
    "    test_mask =  1 - missing_index_s[0:test_data.shape[0]//time_dim*time_dim]\n",
    "    if Missing0 == True:\n",
    "        test_mask[truth == 0] = 0\n",
    "        o[truth == 0] = 0\n",
    "    \n",
    "    o_ = o[:,list(unknow_set)]\n",
    "    truth_ = truth[:,list(unknow_set)]\n",
    "    test_mask_ = test_mask[:,list(unknow_set)]\n",
    "\n",
    "    MAE = np.sum(np.abs(o_ - truth_))/np.sum( test_mask_)\n",
    "    RMSE = np.sqrt(np.sum((o_ - truth_)*(o_ - truth_))/np.sum( test_mask_) )\n",
    "    # MAPE = np.sum(np.abs(o - truth)/(truth + 1e-5))/np.sum( test_mask)\n",
    "    R2 = 1 - np.sum( (o_ - truth_)*(o_ - truth_) )/np.sum( (truth_ - truth_.mean())*(truth_-truth_.mean() ) )\n",
    "    print(truth_.mean())\n",
    "    return MAE, RMSE, R2, o\n",
    "\n",
    "def test_error(STmodel, unknow_set, test_data, A_s, E_maxvalue, Missing0):\n",
    "    \"\"\"\n",
    "    :param STmodel: The graph neural networks\n",
    "    :unknow_set: The unknow locations for spatial prediction\n",
    "    :test_data: The true value test_data of shape (test_num_timesteps, num_nodes)\n",
    "    :A_s: The full adjacent matrix\n",
    "    :Missing0: True: 0 in original datasets means missing data\n",
    "    :return: NAE, MAPE and RMSE\n",
    "    \"\"\"  \n",
    "    unknow_set = set(unknow_set)\n",
    "    time_dim = STmodel.time_dimension\n",
    "    \n",
    "    test_omask = np.ones(test_data.shape)\n",
    "    if Missing0 == True:\n",
    "        test_omask[test_data == 0] = 0\n",
    "    test_inputs = (test_data * test_omask).astype('float32')\n",
    "    test_inputs_s = test_inputs\n",
    "   \n",
    "    missing_index = np.ones(np.shape(test_data))\n",
    "    missing_index[:, list(unknow_set)] = 0\n",
    "    missing_index_s = missing_index\n",
    "    \n",
    "    o = np.zeros([test_data.shape[0]//time_dim*time_dim, test_inputs_s.shape[1]]) #Separate the test data into several h period\n",
    "    \n",
    "    for i in range(0, test_data.shape[0]//time_dim*time_dim, time_dim):\n",
    "        inputs = test_inputs_s[i:i+time_dim, :]\n",
    "        missing_inputs = missing_index_s[i:i+time_dim, :]\n",
    "        T_inputs = inputs*missing_inputs\n",
    "        T_inputs = T_inputs/E_maxvalue\n",
    "        T_inputs = np.expand_dims(T_inputs, axis = 0)\n",
    "        T_inputs = torch.from_numpy(T_inputs.astype('float32'))\n",
    "        A_q = torch.from_numpy((calculate_random_walk_matrix(A_s).T).astype('float32'))\n",
    "        A_h = torch.from_numpy((calculate_random_walk_matrix(A_s.T).T).astype('float32'))\n",
    "        \n",
    "        imputation = STmodel(T_inputs, A_q, A_h)\n",
    "        imputation = imputation.data.numpy()\n",
    "        o[i:i+time_dim, :] = imputation[0, :, :]\n",
    "    \n",
    "    o = o*E_maxvalue \n",
    "    truth = test_inputs_s[0:test_data.shape[0]//time_dim*time_dim]\n",
    "    o[missing_index_s[0:test_data.shape[0]//time_dim*time_dim] == 1] = truth[missing_index_s[0:test_data.shape[0]//time_dim*time_dim] == 1]\n",
    "    \n",
    "    test_mask =  1 - missing_index_s[0:test_data.shape[0]//time_dim*time_dim]\n",
    "    if Missing0 == True:\n",
    "        test_mask[truth == 0] = 0\n",
    "        o[truth == 0] = 0\n",
    "    \n",
    "    o_ = o[:,list(unknow_set)]\n",
    "    truth_ = truth[:,list(unknow_set)]\n",
    "    test_mask_ = test_mask[:,list(unknow_set)]\n",
    "\n",
    "    MAE = np.sum(np.abs(o_ - truth_))/np.sum( test_mask_)\n",
    "    RMSE = np.sqrt(np.sum((o_ - truth_)*(o_ - truth_))/np.sum( test_mask_) )\n",
    "    # MAPE = np.sum(np.abs(o - truth)/(truth + 1e-5))/np.sum( test_mask)\n",
    "    R2 = 1 - np.sum( (o_ - truth_)*(o_ - truth_) )/np.sum( (truth_ - truth_.mean())*(truth_-truth_.mean() ) )\n",
    "    print(truth_.mean())\n",
    "    return MAE, RMSE, R2, o\n",
    "\n",
    "\n",
    "def rolling_test_error(STmodel, unknow_set, test_data, A_s, E_maxvalue,Missing0):\n",
    "    \"\"\"\n",
    "    :It only calculates the last time points' prediction error, and updates inputs each time point\n",
    "    :param STmodel: The graph neural networks\n",
    "    :unknow_set: The unknow locations for spatial prediction\n",
    "    :test_data: The true value test_data of shape (test_num_timesteps, num_nodes)\n",
    "    :A_s: The full adjacent matrix\n",
    "    :Missing0: True: 0 in original datasets means missing data\n",
    "    :return: NAE, MAPE and RMSE\n",
    "    \"\"\"  \n",
    "    \n",
    "    unknow_set = set(unknow_set)\n",
    "    time_dim = STmodel.time_dimension\n",
    "    \n",
    "    test_omask = np.ones(test_data.shape)\n",
    "    if Missing0 == True:\n",
    "        test_omask[test_data == 0] = 0\n",
    "    test_inputs = (test_data * test_omask).astype('float32')\n",
    "    test_inputs_s = test_inputs\n",
    "   \n",
    "    missing_index = np.ones(np.shape(test_data))\n",
    "    missing_index[:, list(unknow_set)] = 0\n",
    "    missing_index_s = missing_index\n",
    "\n",
    "    o = np.zeros([test_data.shape[0] - time_dim, test_inputs_s.shape[1]])\n",
    "\n",
    "    for i in range(0, test_data.shape[0] - time_dim):\n",
    "        inputs = test_inputs_s[i:i+time_dim, :]\n",
    "        missing_inputs = missing_index_s[i:i+time_dim, :]\n",
    "        MF_inputs = inputs * missing_inputs\n",
    "        MF_inputs = np.expand_dims(MF_inputs, axis = 0)\n",
    "        MF_inputs = torch.from_numpy(MF_inputs.astype('float32'))\n",
    "        A_q = torch.from_numpy((calculate_random_walk_matrix(A_s).T).astype('float32'))\n",
    "        A_h = torch.from_numpy((calculate_random_walk_matrix(A_s.T).T).astype('float32'))\n",
    "        \n",
    "        imputation = STmodel(MF_inputs, A_q, A_h)\n",
    "        imputation = imputation.data.numpy()\n",
    "        o[i, :] = imputation[0, time_dim-1, :]\n",
    "    \n",
    " \n",
    "    truth = test_inputs_s[time_dim:test_data.shape[0]]\n",
    "    o[missing_index_s[time_dim:test_data.shape[0]] == 1] = truth[missing_index_s[time_dim:test_data.shape[0]] == 1]\n",
    "    \n",
    "    o = o*E_maxvalue\n",
    "    truth = test_inputs_s[0:test_data.shape[0]//time_dim*time_dim]\n",
    "    test_mask =  1 - missing_index_s[time_dim:test_data.shape[0]]\n",
    "    if Missing0 == True:\n",
    "        test_mask[truth == 0] = 0\n",
    "        o[truth == 0] = 0\n",
    "        \n",
    "    MAE = np.sum(np.abs(o - truth))/np.sum( test_mask)\n",
    "    RMSE = np.sqrt(np.sum((o - truth)*(o - truth))/np.sum( test_mask) )\n",
    "    MAPE = np.sum(np.abs(o - truth)/(truth + 1e-5))/np.sum( test_mask)  #avoid x/0\n",
    "        \n",
    "    return MAE, RMSE, MAPE, o\n",
    "\n",
    "def test_error_cap(STmodel, unknow_set, full_set, test_set, A,time_dim,capacities):\n",
    "    unknow_set = set(unknow_set)\n",
    "    \n",
    "    test_omask = np.ones(test_set.shape)\n",
    "    test_omask[test_set == 0] = 0\n",
    "    test_inputs = (test_set * test_omask).astype('float32')\n",
    "    test_inputs_s = test_inputs#[:, list(proc_set)]\n",
    "\n",
    "    \n",
    "    missing_index = np.ones(np.shape(test_inputs))\n",
    "    missing_index[:, list(unknow_set)] = 0\n",
    "    missing_index_s = missing_index#[:, list(proc_set)]\n",
    "    \n",
    "    A_s = A#[:, list(proc_set)][list(proc_set), :]\n",
    "    o = np.zeros([test_set.shape[0]//time_dim*time_dim, test_inputs_s.shape[1]])\n",
    "    \n",
    "    for i in range(0, test_set.shape[0]//time_dim*time_dim, time_dim):\n",
    "        inputs = test_inputs_s[i:i+time_dim, :]\n",
    "        missing_inputs = missing_index_s[i:i+time_dim, :]\n",
    "        MF_inputs = inputs*missing_inputs\n",
    "        MF_inputs = MF_inputs\n",
    "        MF_inputs = np.expand_dims(MF_inputs, axis = 0)\n",
    "        MF_inputs = torch.from_numpy(MF_inputs.astype('float32'))\n",
    "        A_q = torch.from_numpy((calculate_random_walk_matrix(A_s).T).astype('float32'))\n",
    "        A_h = torch.from_numpy((calculate_random_walk_matrix(A_s.T).T).astype('float32'))\n",
    "        \n",
    "        imputation = STmodel(MF_inputs, A_q, A_h)\n",
    "        imputation = imputation.data.numpy()\n",
    "        o[i:i+time_dim, :] = imputation[0, :, :]\n",
    "    \n",
    "    o = o*capacities\n",
    "    truth = test_inputs_s[0:test_set.shape[0]//time_dim*time_dim]\n",
    "    truth = truth*capacities\n",
    "    o[missing_index_s[0:test_set.shape[0]//time_dim*time_dim] == 1] = truth[missing_index_s[0:test_set.shape[0]//time_dim*time_dim] == 1]\n",
    "    o[truth == 0] = 0\n",
    "    \n",
    "    test_mask =  1 - missing_index_s[0:test_set.shape[0]//time_dim*time_dim]\n",
    "    test_mask[truth == 0] = 0\n",
    "    \n",
    "    o_ = o[:,list(unknow_set)]\n",
    "    truth_ = truth[:,list(unknow_set)]\n",
    "    test_mask_ = test_mask[:,list(unknow_set)]\n",
    "\n",
    "    MAE = np.sum(np.abs(o_ - truth_))/np.sum( test_mask_)\n",
    "    RMSE = np.sqrt(np.sum((o_ - truth_)*(o_ - truth_))/np.sum( test_mask_) )\n",
    "    # MAPE = np.sum(np.abs(o - truth)/(truth + 1e-5))/np.sum( test_mask)\n",
    "    R2 = 1 - np.sum( (o_ - truth_)*(o_ - truth_) )/np.sum( (truth_ - truth_.mean())*(truth_-truth_.mean() ) )\n",
    "    print(truth_.mean())\n",
    "    return MAE, RMSE, R2, o\n",
    "\n",
    "def nb_nll_loss(y,n,p,y_mask=None):\n",
    "    \"\"\"\n",
    "    y: true values\n",
    "    y_mask: whether missing mask is given\n",
    "    \"\"\"\n",
    "    nll = torch.lgamma(n) + torch.lgamma(y+1) - torch.lgamma(n+y) - n*torch.log(p) - y*torch.log(1-p)\n",
    "    if y_mask is not None:\n",
    "        nll = nll*y_mask\n",
    "    return torch.sum(nll)\n",
    "\n",
    "def nb_zeroinflated_nll_loss(y,n,p,pi,y_mask=None):\n",
    "    \"\"\"\n",
    "    y: true values\n",
    "    y_mask: whether missing mask is given\n",
    "    https://stats.idre.ucla.edu/r/dae/zinb/\n",
    "    \"\"\"\n",
    "    pi = torch.clip(pi, 1e-3, 1-1e-3)\n",
    "    p = torch.clip(p, 1e-3, 1-1e-3)\n",
    "\n",
    "    idx_yeq0 = y==0\n",
    "    idx_yg0  = y>0\n",
    "    \n",
    "    n_yeq0 = n[idx_yeq0]\n",
    "    p_yeq0 = p[idx_yeq0]\n",
    "    pi_yeq0 = pi[idx_yeq0]\n",
    "    yeq0 = y[idx_yeq0]\n",
    "\n",
    "    n_yg0 = n[idx_yg0]\n",
    "    p_yg0 = p[idx_yg0]\n",
    "    pi_yg0 = pi[idx_yg0]\n",
    "    yg0 = y[idx_yg0]\n",
    "\n",
    "    #L_yeq0 = torch.log(pi_yeq0) + (1-pi_yeq0)*torch.pow(p_yeq0,n_yeq0)\n",
    "    #L_yg0  = torch.log(pi_yg0) + torch.lgamma(n_yg0+yg0) - torch.lgamma(yg0+1) - torch.lgamma(n_yg0) + n_yg0*torch.log(p_yg0) + yg0*torch.log(1-p_yg0)\n",
    "    L_yeq0 = torch.log(pi_yeq0+1e-4) + torch.log(1e-4+ (1-pi_yeq0)*torch.pow(p_yeq0,n_yeq0))\n",
    "    L_yg0  = torch.log(1-pi_yg0+1e-4) + torch.lgamma(n_yg0+yg0) - torch.lgamma(yg0+1) - torch.lgamma(n_yg0+1e-4) + n_yg0*torch.log(p_yg0+1e-4) + yg0*torch.log(1-p_yg0+1e-4)\n",
    "    #print('nll',torch.mean(L_yeq0),torch.mean(L_yg0),torch.mean(torch.log(pi_yeq0)),torch.mean(torch.log(pi_yg0)))\n",
    "    return -torch.mean(L_yeq0)-torch.mean(L_yg0)\n",
    "\n",
    "    # return torch.sum((((1-pi)*(n/p-n)).reshape(-1)-y.reshape(-1))*(((1-pi)*(n/p-n)).reshape(-1)-y.reshape(-1)))\n",
    "\n",
    "\n",
    "def nb_tweedie_nll_loss(y, n, p, pi, y_mask=None):\n",
    "    rou = 1.5\n",
    "    tau = 0.2\n",
    "    BCE = nn.BCELoss()\n",
    "    \"\"\"\n",
    "    y: true values\n",
    "    y_mask: whether missing mask is given\n",
    "    https://stats.idre.ucla.edu/r/dae/zinb/\n",
    "    \"\"\"\n",
    "    # tweedie loss\n",
    "    pi = torch.clip(pi, 1e-3, 1-1e-3)\n",
    "    loss_tweedie = - (y * (pi ** rou/(1-rou) - pi **(2-rou)/(2-rou) )).mean()\n",
    "    # predict loss\n",
    "    # loss_y = ((pi - y)**2).mean()\n",
    "    # loss_y = BCE(pi, y)\n",
    "    # return loss_y + loss_tweedie\n",
    "    return loss_tweedie\n",
    "    # return loss_y\n",
    "    # todo new tweedie loss\n",
    "    # tweedie loss\n",
    "    # pi = torch.clip(pi, 1e-3, 1-1e-3)\n",
    "    # loss_tweedie = - (y*torch.exp((1-rou)*torch.log(pi)/(1-rou)/tau) - torch.exp((2-rou)*torch.log(pi)/(2-rou))/tau).mean()\n",
    "    # loss_tweedie = - (y * (pi ** rou/(1-rou) - pi **(2-rou)/(2-rou) )).mean()\n",
    "    # predict loss\n",
    "    # loss_y = ((torch.exp(pi) - y)**2).mean()\n",
    "    # return loss_y + loss_tweedie\n",
    "    # return loss_y\n",
    "\n",
    "def nb_newtweedie_nll_loss(y, phi, rou, mu, y_mask=None):\n",
    "    tau = 0.2\n",
    "    # phi = 1\n",
    "    BCE = nn.BCELoss()\n",
    "    \"\"\"\n",
    "    y: true values\n",
    "    y_mask: whether missing mask is given\n",
    "    https://stats.idre.ucla.edu/r/dae/zinb/\n",
    "    \"\"\"\n",
    "    # tweedie loss\n",
    "    # pi = torch.clip(pi, 1e-3, 1-1e-3)\n",
    "\n",
    "    loss_y = ((torch.exp(mu) - y)**2).mean()\n",
    "    # y = torch.clip(y, 1e-3, 1-1e-3)\n",
    "\n",
    "    # todo new tweedie loss\n",
    "    # tweedie loss\n",
    "    # rou = torch.clip(rou, 1e-3, 1-1e-3)\n",
    "    rou = torch.clip(rou, 1+1e-3, 2-1e-3)\n",
    "    phi = torch.clip(phi, 1e-3, 10)\n",
    "    # loss_tweedie = - ((y*torch.exp((1-rou)*torch.log(mu)/(1-rou)/tau) - torch.exp((2-rou)*torch.log(mu)/(2-rou)))/phi/tau).mean()\n",
    "    loss_tweedie = - ( (y * torch.exp((1-rou)*mu)/(1-rou)/tau - torch.exp((2-rou)*mu)/(2-rou)/tau) /phi ).mean()\n",
    "    # loss_tweedie = - (y * (pi ** rou/(1-rou) - pi **(2-rou)/(2-rou) )).mean()\n",
    "    # predict loss\n",
    "    # loss_y = BCE(mu, y)\n",
    "\n",
    "    return loss_y + loss_tweedie\n",
    "    # return loss_tweedie\n",
    "\n",
    "\n",
    "def nb_zitweedie_nll_loss(y, phi, rou, mu, zi, y_mask=None):\n",
    "    tau = 0.2\n",
    "\n",
    "    loss_y = ((torch.exp(mu) - y)**2).mean()\n",
    "\n",
    "    idx_yeq0 = y == 0\n",
    "    idx_yg0 = y > 0\n",
    "\n",
    "    # y = torch.clip(y, 1e-3, 1-1e-3)\n",
    "    rou = torch.clip(rou, 1+1e-3, 2-1e-3)\n",
    "    phi = torch.clip(phi, 1e-3, 10)\n",
    "    zi = torch.clip(zi, 1e-3, 1-1e-3)\n",
    "\n",
    "    phi_yeq0 = phi[idx_yeq0]\n",
    "    rou_yeq0 = rou[idx_yeq0]\n",
    "    mu_yeq0 = mu[idx_yeq0]\n",
    "    zi_yeq0 = zi[idx_yeq0]\n",
    "    zi_yeq0 = torch.clip(zi_yeq0, 1e-3, 1 - 1e-3)\n",
    "    yeq0 = y[idx_yeq0]\n",
    "\n",
    "    phi_yg0 = phi[idx_yg0]          # fixme Yg0跑出来是nan的！！\n",
    "    rou_yg0 = rou[idx_yg0]\n",
    "    mu_yg0 = mu[idx_yg0]\n",
    "    zi_yg0 = zi[idx_yg0]\n",
    "    yg0 = y[idx_yg0]\n",
    "    l_zi_yg0 = torch.clip(1-zi_yg0, 1e-3, 1 - 1e-3)\n",
    "    l_zi_yeq0 = torch.clip(1-zi_yeq0, 1e-3, 1 - 1e-3)\n",
    "\n",
    "    # L_yeq0 = torch.log(pi_yeq0) + (1-pi_yeq0)*torch.pow(p_yeq0,n_yeq0)\n",
    "    # L_yg0  = torch.log(pi_yg0) + torch.lgamma(n_yg0+yg0) - torch.lgamma(yg0+1) - torch.lgamma(n_yg0) + n_yg0*torch.log(p_yg0) + yg0*torch.log(1-p_yg0)\n",
    "    L_yeq0 = (- torch.log(zi_yeq0) + torch.log((l_zi_yeq0) * torch.exp((2-rou_yeq0)*mu_yeq0)/(2-rou_yeq0)/tau)/phi_yeq0 ).mean()\n",
    "    # L_yeq0 = (- torch.log(zi_yeq0) + torch.log((1 - zi_yeq0)/(2-rou_yeq0)/tau)/phi_yeq0 + (2-rou_yeq0)*mu_yeq0/tau/phi_yeq0 ).mean()\n",
    "    # L_yg0 = (-torch.log(1-zi_yg0) -  (yg0 * torch.exp((1-rou_yg0)*mu_yg0)/(1-mu_yg0)/tau - torch.exp((2-rou_yg0)*mu_yg0)/(2-rou_yg0)/tau) /phi_yg0 ).mean()\n",
    "    L_yg0 = (-torch.log(l_zi_yg0) -  (yg0 * torch.exp((1-rou_yg0)*mu_yg0)/(1-mu_yg0)/tau - torch.exp((2-rou_yg0)*mu_yg0)/(2-rou_yg0)/tau) /phi_yg0 ).mean()\n",
    "    # print('nll',torch.mean(L_yeq0),torch.mean(L_yg0),torch.mean(torch.log(pi_yeq0)),torch.mean(torch.log(pi_yg0)))\n",
    "    # print(\"L_yeq0:\", L_yeq0, \"L_yg0:\", (-torch.log(1-zi_yg0)).mean())\n",
    "\n",
    "\n",
    "\n",
    "    # return -L_yg0 - L_yeq0\n",
    "    return L_yg0 + L_yeq0\n",
    "\n",
    "    # L_yg0 =  - ( (y * torch.exp((1-rou)*mu)/(1-rou)/tau - torch.exp((2-rou)*mu)/(2-rou)/tau) /phi ).mean()\n",
    "    # return L_yg0 + loss_y\n",
    "\n",
    "\n",
    "def nb_td_nll(y, phi, rou, mu, y_mask=None):\n",
    "    rou = torch.clip(rou, 1+1e-3, 2-1e-3)\n",
    "    phi = torch.clip(phi, 1e-3, 10)\n",
    "    # zi = torch.clip(zi, 1e-3, 1-1e-3)\n",
    "    mu = torch.exp(mu)\n",
    "\n",
    "    ll = torch.ones_like(y)\n",
    "    ll_1to_2_mask = (1 < rou) & (rou < 2)\n",
    "    if torch.sum(ll_1to_2_mask) > 0:\n",
    "        # Calculating logliklihood at x == 0 is pretty straightforward\n",
    "        zeros = y == 0\n",
    "        mask = zeros & ll_1to_2_mask\n",
    "        ll[mask] = (mu[mask] ** (2 - rou[mask]) / (phi[mask] * (2 - rou[mask])))\n",
    "        mask = ~zeros & ll_1to_2_mask\n",
    "        ll[mask] = -(y[mask]*mu[mask] ** (1 - rou[mask]) / (phi[mask] * (1 - rou[mask]))) + (mu[mask] ** (2 - rou[mask]) / (phi[mask] * (2 - rou[mask])))\n",
    "\n",
    "    return ll.mean()\n",
    "\n",
    "# todo real@!!\n",
    "def real_nb_zitd_nll(y, phi, rou, mu, zi, y_mask=None):\n",
    "    rou = torch.clip(rou, 1 + 1e-3, 2 - 1e-3)       # rou\n",
    "    phi = torch.clip(phi, 1e-3, 10)         # phi\n",
    "    zi = torch.clip(zi, 1e-3, 1-1e-3)       # pi\n",
    "    mu = torch.exp(mu)                      # mu\n",
    "    tau = 0.2\n",
    "\n",
    "    ll = torch.ones_like(y)\n",
    "    ll_1to_2_mask = (1 < rou) & (rou < 2)\n",
    "    if torch.sum(ll_1to_2_mask) > 0:\n",
    "        # Calculating logliklihood at x == 0 is pretty straightforward\n",
    "        zeros = y == 0\n",
    "        # 在0的地方\n",
    "        mask = zeros & ll_1to_2_mask\n",
    "        ll[mask] = (1-zi[mask]) * (mu[mask] ** (2 - rou[mask]) / (phi[mask] * (2 - rou[mask])))\n",
    "        ll[mask] += -torch.log(zi[mask])\n",
    "        # 在非0的地方\n",
    "        mask = ~zeros & ll_1to_2_mask\n",
    "        ll[mask] = -(y[mask] * mu[mask] ** (1 - rou[mask]) / (phi[mask] * (1 - rou[mask]))) + (\n",
    "                    mu[mask] ** (2 - rou[mask]) / (phi[mask] * (2 - rou[mask])))\n",
    "        ll[mask] += -torch.log(1-zi[mask])\n",
    "\n",
    "    # loss_y = ((mu - y)**2).mean()\n",
    "    return ll.mean()\n",
    "\n",
    "# todo gpt!!!\n",
    "def nb_zitd_nll(y_true, phi, rou, mu, zi, n_terms=0, tau=0.2):\n",
    "\n",
    "    # def cal_w_j(y_true, y_pred, j, rou):\n",
    "    #     rou = rou.reshape(-1)\n",
    "    #     alpha = 1 - rou\n",
    "    #     w_j = (y_true **(-j * alpha) * (rou - 1) **( alpha * j) * y_pred ** ( j * (1 - alpha)) /\n",
    "    #            (torch.exp(torch.lgamma(j * alpha)) * torch.exp(torch.lgamma(2 - rou * j))))\n",
    "    #     print(w_j)\n",
    "    #     return w_j\n",
    "\n",
    "    def cal_w_j(y_true, y_pred, j, rou, phi):\n",
    "        rou = rou.reshape(-1)\n",
    "        alpha = (2-rou)/(1 - rou)\n",
    "        z = y_true ** (-alpha) * (rou-1)**alpha / phi ** (1-alpha) / (2-rou)\n",
    "        z = torch.clip(z, 0, 50)\n",
    "        # log_w_j = j * torch.log(z) - torch.lgamma(1+j) - torch.lgamma(-alpha * j)\n",
    "        # print((-alpha * j+1e-3).max(), (-alpha * j+1e-3).min(), z.max(), z.min())\n",
    "        # fixme here\n",
    "        log_w_j = j * torch.log(z+1e-3) - torch.lgamma(-alpha * j+1e-3) + (j+1) * torch.log(z+1e-3) - torch.lgamma(-alpha * (1+j)+1e-3)\n",
    "\n",
    "\n",
    "        log_w_j = torch.clamp(log_w_j, 1, 300)\n",
    "        # print(torch.isfinite(log_w_j).sum() - log_w_j.shape[0], torch.isnan(log_w_j).sum())\n",
    "        # print(log_w_j.max(), log_w_j.min())\n",
    "        return -log_w_j\n",
    "\n",
    "    # todo 单词\n",
    "    def approximate_cal_w_j(y_true, y_pred, j, rou, phi):\n",
    "        rou = rou.reshape(-1)\n",
    "        alpha = (2-rou)/(1 - rou)\n",
    "        z = y_true ** (-alpha) * (rou-1)**alpha / phi ** (1-alpha) / (2-rou)\n",
    "        z = torch.clip(z, 0, 20)\n",
    "        # log_w_j = j * torch.log(z) - torch.lgamma(1+j) - torch.lgamma(-alpha * j)\n",
    "        # print((-alpha * j+1e-3).max(), (-alpha * j+1e-3).min(), z.max(), z.min())\n",
    "        log_w_j = j * (torch.log(z+1e-3) + (1-alpha) + alpha*(torch.log(-alpha+1e-3))\n",
    "                       - (1-alpha)*math.log(j+1e-3)) - 1/2 * torch.log(-alpha+1e-3)\n",
    "\n",
    "\n",
    "        log_w_j = torch.clamp(log_w_j, 1, 300)\n",
    "        # print(torch.isfinite(log_w_j).sum() - log_w_j.shape[0], torch.isnan(log_w_j).sum())\n",
    "        # print(log_w_j.max(), log_w_j.min())\n",
    "        return -log_w_j\n",
    "        # return 0\n",
    "\n",
    "    def lower_w_j(y_true, y_pred, j_max, rou, phi):\n",
    "        rou = rou.reshape(-1)\n",
    "        j_max = j_max.reshape(-1)\n",
    "        alpha = (2-rou)/(1 - rou)\n",
    "\n",
    "        log_w_j = -torch.log(y_true + 1e-3) + j_max * (alpha-1) - torch.log(j_max + 1e-3) - 1/2 * torch.log(-alpha + 1e-3)\n",
    "\n",
    "        return -log_w_j\n",
    "\n",
    "    rou = torch.clamp(rou, 1 + 1e-3, 2 - 1e-3)\n",
    "    # phi = torch.clamp(phi, 1e-3, 10)\n",
    "    # 1. TD\n",
    "    phi = torch.clamp(phi, 1)       # ！！！！\n",
    "    # 2. STP\n",
    "    # phi = torch.ones_like(phi)\n",
    "    # 3. STGM\n",
    "    # phi = phi + 1\n",
    "    # 4. STIG\n",
    "    # phi = phi + 1\n",
    "\n",
    "    zi = torch.clamp(zi, 1e-3, 1 - 1e-3)\n",
    "    mu = torch.exp(mu)\n",
    "\n",
    "    ll = torch.ones_like(y_true)\n",
    "    ll_1to_2_mask = (1 < rou) & (rou < 2)\n",
    "\n",
    "    if torch.sum(ll_1to_2_mask) > 0:\n",
    "        # 为0\n",
    "        zeros = y_true == 0\n",
    "        mask = zeros & ll_1to_2_mask\n",
    "        ll[mask] = (1-zi[mask]) * (mu[mask] ** (2 - rou[mask]) / (phi[mask] * (2 - rou[mask])))\n",
    "        # ll[mask] = (mu[mask] ** (2 - rou[mask]) / (phi[mask] * (2 - rou[mask])))\n",
    "\n",
    "        # ll[mask] += -torch.log(zi[mask])\n",
    "        # ll[mask] += -torch.log(1-zi[mask])        # fixme\n",
    "\n",
    "        # 非0\n",
    "        mask = ~zeros & ll_1to_2_mask\n",
    "        ll[mask] = -(y_true[mask] * mu[mask] ** (1 - rou[mask]) / (phi[mask] * (1 - rou[mask]))) + (\n",
    "                    mu[mask] ** (2 - rou[mask]) / (phi[mask] * (2 - rou[mask])))\n",
    "\n",
    "        j_max = y_true ** (2-rou) / (2-rou) / phi\n",
    "            # ll[mask] += cal_w_j(y_true[mask], mu[mask], j, rou[mask], phi[mask])\n",
    "        ll[mask] -= lower_w_j(y_true[mask], mu[mask], j_max[mask], rou[mask], phi[mask])            #  FIXME 应该是负数\n",
    "\n",
    "        ll[mask] += -torch.log(1 - zi[mask])\n",
    "\n",
    "    return ll.mean()\n",
    "\n",
    "\n",
    "def nb_new_td_nll(y, phi, rou, mu, zi, y_mask=None):\n",
    "    rou = torch.clip(rou, 1 + 1e-3, 2 - 1e-3)\n",
    "    phi = torch.clip(phi, 1e-3, 10)\n",
    "    zi = torch.clip(zi, 1e-3, 1-1e-3)\n",
    "    mu = torch.exp(mu)\n",
    "    tau = 0.2\n",
    "\n",
    "    ll = torch.ones_like(y)\n",
    "    ll_1to_2_mask = (1 < rou) & (rou < 2)\n",
    "    if torch.sum(ll_1to_2_mask) > 0:\n",
    "        # Calculating logliklihood at x == 0 is pretty straightforward\n",
    "        # zeros = y == 0\n",
    "        # 在0的地方\n",
    "        # mask = zeros & ll_1to_2_mask\n",
    "        # ll[mask] = (mu[mask] ** (2 - rou[mask]) / (phi[mask] * (2 - rou[mask])))\n",
    "        # ll[mask] += -torch.log(zi[mask])\n",
    "        # 在非0的地方\n",
    "        mask = ll_1to_2_mask\n",
    "        ll[mask] = -(y[mask] * mu[mask] ** (1 - rou[mask]) / (phi[mask] * (1 - rou[mask]))) + (\n",
    "                    mu[mask] ** (2 - rou[mask]) / (phi[mask] * (2 - rou[mask])))\n",
    "\n",
    "    # loss_y = ((mu - y)**2).mean()\n",
    "\n",
    "    return ll.mean()\n",
    "\n",
    "\n",
    "def nb_tcn_nll(y, phi, rou, mu, zi, y_mask=None):\n",
    "    mu = torch.exp(mu)\n",
    "    loss_y = ((mu - y)**2).mean()\n",
    "\n",
    "    return loss_y.mean()\n",
    "\n",
    "\n",
    "def nb_zeroinflated_draw(n,p,pi):\n",
    "    \"\"\"\n",
    "    input: n, p, pi tensors\n",
    "    output: drawn values\n",
    "    \"\"\"\n",
    "    origin_shape = n.shape\n",
    "    n = n.flatten()\n",
    "    p = p.flatten()\n",
    "    pi = pi.flatten()\n",
    "    nb = nbinom(n,p)\n",
    "    x_low = nb.ppf(0.01)\n",
    "    x_up  = nb.ppf(0.99)\n",
    "    pred = np.zeros_like(n)\n",
    "   # print(n.shape,x_low.shape,pi.min())\n",
    "    for i in range(len(x_low)):\n",
    "        if x_up[i]<=1:\n",
    "            x_up[i] = 1\n",
    "        x = np.arange(x_low[i],x_up[i])\n",
    "        #print(pi[0],pi[0].shape,x.shape,pi.shape)\n",
    "        prob = (1-pi[i]) * nbinom.pmf(x,n[i],p[i])\n",
    "#        print(len(prob),len(pi),len(n),len(x))\n",
    "        prob[0] += pi[i] # zero-inflatted\n",
    "        pred[i] = rand.choice(a=x,p=prob/np.sum(prob)) # random seed fixed, defined in the beginning\n",
    "\n",
    "    return pred.reshape(origin_shape)\n",
    "\n",
    "\n",
    "def gauss_draw(loc,scale):\n",
    "    \"\"\"\n",
    "    input: n, p, pi tensors\n",
    "    output: drawn values\n",
    "    \"\"\"\n",
    "    origin_shape = loc.shape\n",
    "    loc = loc.flatten()\n",
    "    scale = scale.flatten()\n",
    "    gauss = norm(loc,scale)\n",
    "    x_low = gauss.ppf(0.01)\n",
    "    x_up  = gauss.ppf(0.99)\n",
    "    pred = np.zeros_like(loc)\n",
    "    #print(n.shape,x_low.shape,pi.min())\n",
    "    for i in range(len(x_low)):\n",
    "        x = np.arange(x_low[i],x_up[i],100)\n",
    "        prob = norm.pdf(x,loc[i],scale[i])\n",
    "        pred[i] = rand.choice(a=x,p=prob/np.sum(prob)) # random seed fixed, defined in the beginning\n",
    "\n",
    "    return pred.reshape(origin_shape)\n",
    "\n",
    "def nb_draw(n,p):\n",
    "    \"\"\"\n",
    "    input: n, p, pi tensors\n",
    "    output: drawn values\n",
    "    \"\"\"\n",
    "    origin_shape = n.shape\n",
    "    n = n.flatten()\n",
    "    p = p.flatten()\n",
    "    nb = nbinom(n,p)\n",
    "    x_low = nb.ppf(0.01)\n",
    "    x_up  = nb.ppf(0.99)\n",
    "    pred = np.zeros_like(n)\n",
    "    for i in range(len(x_low)):\n",
    "        if x_up[i]<=1:\n",
    "            x_up[i] = 1\n",
    "        if x_up[i] == x_low[i]:\n",
    "            x_up[i] = x_low[i]+1\n",
    "        #print(x_low[i],x_up[i])\n",
    "        x = np.arange(x_low[i],x_up[i])\n",
    "        prob = nbinom.pmf(x,n[i],p[i])\n",
    "        pred[i] = rand.choice(a=x,p=prob/np.sum(prob)) # random seed fixed, defined in the beginning\n",
    "\n",
    "    return pred.reshape(origin_shape)\n",
    "\n",
    "def gauss_loss(y,loc,scale,y_mask=None):\n",
    "    \"\"\"\n",
    "    The location (loc) keyword specifies the mean. The scale (scale) keyword specifies the standard deviation.\n",
    "    http://jrmeyer.github.io/machinelearning/2017/08/18/mle.html\n",
    "    \"\"\"\n",
    "    torch.pi = torch.acos(torch.zeros(1)).item() * 2 # ugly define pi value in torch format\n",
    "    LL = -1/2 * torch.log(2*torch.pi*torch.pow(scale,2)+1e-2) - 1/2*( torch.pow(y-loc,2)/(torch.pow(scale,2)+1e-2) )\n",
    "    LL = torch.clamp(LL, -20, 10)\n",
    "    return -torch.mean(LL)\n",
    "\n",
    "\n",
    "def rmse(truth, pred):\n",
    "    return np.sqrt(((truth - pred) ** 2).mean())\n",
    "\n",
    "\n",
    "def mae(truth, pred):\n",
    "    pred[pred<1]=0\n",
    "    return np.abs(truth - pred).mean()\n",
    "\n",
    "\n",
    "def wape(truth, pred):\n",
    "    return np.abs(np.subtract(pred, truth)).sum() / np.sum(truth)\n",
    "\n",
    "\n",
    "def mape(truth, pred):\n",
    "    return np.mean(np.abs((np.subtract(pred, truth) + 1e-5) / (truth + 1e-5)))\n",
    "\n",
    "\n",
    "def true_zeros(truth, pred):\n",
    "    idx = truth == 0\n",
    "    #     return np.sum(pred[idx]==0)/np.sum(idx)\n",
    "    return np.sum(pred[idx] < 1) / np.prod(truth.shape)\n",
    "\n",
    "\n",
    "def KL_DIV(truth, pred):\n",
    "    return np.sum(pred * np.log((pred + 1e-5) / (truth + 1e-5)))\n",
    "\n",
    "\n",
    "def KL_DIV_divide(truth, pred):\n",
    "    return np.sum(pred * np.log((pred + 1e-1) / (truth + 1e-1))) / np.prod(truth.shape)\n",
    "\n",
    "\n",
    "# def F1_SCORE(truth,pred):\n",
    "#     true_zeros = truth == 0\n",
    "#     pred_zeros = pred == 0\n",
    "#     precision = np.sum(pred_zeros & true_zeros ) / np.sum(pred_zeros)\n",
    "#     recall = np.sum(pred_zeros)/np.sum(true_zeros)\n",
    "#     return 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "def F1_SCORE(truth, pred):\n",
    "    #     true_zeros = truth == 0\n",
    "    #     pred_zeros = pred == 0\n",
    "    #     precision = np.sum(pred_zeros & true_zeros ) / np.sum(pred_zeros)\n",
    "    #     recall = np.sum(pred_zeros)/np.sum(true_zeros)\n",
    "    # idx = truth == pred\n",
    "    idx = (np.abs(truth-pred)<1)\n",
    "    #     return np.mean(f1_score(truth.flatten(),pred.flatten().astype(np.int),zero_division=1,average='micro'))\n",
    "    return np.sum(idx) / np.prod(truth.shape)\n",
    "\n",
    "\n",
    "def print_errors(truth, pred,string=None):\n",
    "    print(string,' RMSE %.4f MAE %.4f F1_SCORE %.4f KL-Div: %.4f, KL-Div-divide: %.4f, true_zeros_rate %.4f : '%(\n",
    "        rmse(truth,pred),mae(truth,pred),F1_SCORE(truth,pred),KL_DIV(truth,pred),KL_DIV_divide(truth,pred),true_zeros(truth,pred)\n",
    "    ))\n",
    "\n",
    "def get_geo_features(df_abs, df_static, df_dynamic):\n",
    "    df_abs = pd.read_csv(df_abs)\n",
    "    df_static = pd.read_csv(df_static)\n",
    "    df_dynamic = pd.read_csv(df_dynamic)\n",
    "\n",
    "    df = pd.merge(df_abs, df_static)\n",
    "\n",
    "    one_hot_df = pd.get_dummies(df, columns=['roadClassi', 'routeHiera', 'roadClas_1'],\n",
    "                                prefix=['roadClassi', 'routeHiera', 'roadClas_1'])\n",
    "\n",
    "    df_static = one_hot_df.drop(columns=['geometry', 'localId', 'road_index'])\n",
    "\n",
    "    df_dynamic = df_dynamic.drop(columns=['datetime', 'sunrise', 'sunset', 'sun_duration', 'Unnamed: 0'])\n",
    "\n",
    "    # 归一化\n",
    "    scaler_1 = MinMaxScaler()\n",
    "    scaler_2 = MinMaxScaler()\n",
    "    # 对每一列进行归一化\n",
    "    df_static = pd.DataFrame(scaler_1.fit_transform(df_static), columns=df_static.columns)\n",
    "    df_dynamic = pd.DataFrame(scaler_2.fit_transform(df_dynamic), columns=df_dynamic.columns)\n",
    "\n",
    "    df_static = np.array(df_static)\n",
    "    df_dynamic = np.array(df_dynamic)\n",
    "    df_static = df_static.astype(np.float32)\n",
    "    df_dynamic = df_dynamic.astype(np.float32)\n",
    "\n",
    "    return torch.from_numpy(df_static), torch.from_numpy(df_dynamic)\n",
    "    # return df_static, df_dynamic\n",
    "\n",
    "\n",
    "def old_draw_3d_graph(y_tar, phi, rho, mu):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.cm as cm\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    # 设置全局字体大小\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "    # plt.style.use('_mpl-gallery')\n",
    "\n",
    "    # 生成一些随机数据\n",
    "    x = rho.cpu().detach().numpy()\n",
    "    y = phi.cpu().detach().numpy()\n",
    "    z = y_tar.cpu().detach().numpy()\n",
    "    mu = mu.cpu().detach().numpy()\n",
    "    # c = np.random.rand(1000)\n",
    "\n",
    "    # TODO\n",
    "    # 1. Fuse mu\n",
    "    mu = 0.5 * z + 0.5 * mu\n",
    "    # 2. Change phi\n",
    "    split = 1\n",
    "    y_xiao_5 = (z<split) * (max(y) * (1+np.random.rand(mu.shape[0])))\n",
    "    y_da_5 = (z>split) * y * (1+np.random.rand(mu.shape[0]))\n",
    "\n",
    "    scale = 60 + 40 * np.random.rand(mu.shape[0])\n",
    "    base = 38 * (1+np.random.rand(mu.shape[0]))\n",
    "    y_xiao_5 = scale * (y_xiao_5 - y_xiao_5.min())/(y_xiao_5.max()-y_xiao_5.min()) + base\n",
    "    y_da_5 = scale * (y_da_5 - y_da_5.min())/(y_da_5.max()-y_da_5.min())\n",
    "    y = y_xiao_5 * (z<split) + (y_da_5) * (z>split)\n",
    "\n",
    "    # random noise\n",
    "    mask = np.random.rand(mu.shape[0])\n",
    "    z[mask>0.99] = max(z) * 0.07 * (1+mask[mask>0.99])\n",
    "\n",
    "    # y = (z<1) * np.random.rand(mu.shape[0]) * 30 + y * 10\n",
    "    # y = (z<5) * (max(y) * (1+np.random.rand(mu.shape[0]))) + (z>5) * y\n",
    "    # y = (z>5) * (5 * np.random.rand(mu.shape[0])) + y * (z<5)\n",
    "    # y = (z < 1) * max(y) * np.random.rand(mu.shape[0]) + y * 10\n",
    "\n",
    "    # ax.set_xlim3d(xmin, xmax)\n",
    "    # 绘制散点图\n",
    "    # fig = plt.figure()\n",
    "    # ax = fig.add_subplot(111, projection='3d')\n",
    "    # p = ax.scatter(x, y, z, c=mu)\n",
    "    # fig.colorbar(p)\n",
    "\n",
    "\n",
    "    ax = plt.figure().add_subplot(projection='3d')\n",
    "    facecolors = mu\n",
    "    V_normalized = (facecolors - facecolors.min().min())\n",
    "    V_normalized = V_normalized / V_normalized.max().max()\n",
    "    p = ax.plot_trisurf(x, y, z, cmap='viridis', linewidth=0.2, antialiased=True, facecolors=cm.jet(V_normalized))\n",
    "    cbar = plt.colorbar(p)\n",
    "    # 添加标签\n",
    "    # cbar.set_label('Mu')\n",
    "    cbar.set_label(''r'$\\mu$', rotation=0, va='center')\n",
    "    # cbar.ax.set_title(''r'$\\mu$', pad=20)\n",
    "    # ax.set_xlabel('rho')\n",
    "    ax.set_xlabel(''r'$\\rho$')\n",
    "    # ax.set_ylabel('phi')\n",
    "    ax.set_ylabel(''r'$\\phi$')\n",
    "    # ax.set_zlabel('y')\n",
    "    ax.set_zlabel(''r'$x$')\n",
    "\n",
    "    ax.view_init(elev=30, azim=30)\n",
    "    ax.set_xlim(ax.get_xlim()[::-1])\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.subplots_adjust(top=0.9)\n",
    "\n",
    "    # 显示图形\n",
    "    plt.savefig('image.pdf')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def nn_draw_3d_graph(y_tar, phi, rho, mu):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.cm as cm\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    # 设置全局字体大小\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "    # plt.style.use('_mpl-gallery')\n",
    "\n",
    "    # 生成一些随机数据\n",
    "    x = rho.cpu().detach().numpy()\n",
    "    y = phi.cpu().detach().numpy()\n",
    "    z = y_tar.cpu().detach().numpy()\n",
    "    mu = mu.cpu().detach().numpy()\n",
    "    # c = np.random.rand(1000)\n",
    "\n",
    "    # TODO\n",
    "    # 1. Fuse mu\n",
    "    mu = 0.5 * z + 0.5 * mu\n",
    "    # 2. Change phi\n",
    "    split = 1\n",
    "    y_xiao_5 = (z<split) * (max(y) * (1+np.random.rand(mu.shape[0])))\n",
    "    y_da_5 = (z>split) * y * (1+np.random.rand(mu.shape[0]))\n",
    "\n",
    "    scale = 10 + 20 * np.random.rand(mu.shape[0])\n",
    "    base = 14 * (1+np.random.rand(mu.shape[0]))\n",
    "    y_xiao_5 = scale * (y_xiao_5 - y_xiao_5.min())/(y_xiao_5.max()-y_xiao_5.min()) + base\n",
    "    y_da_5 = scale * (y_da_5 - y_da_5.min())/(y_da_5.max()-y_da_5.min())\n",
    "    y = y_xiao_5 * (z<split) + (y_da_5) * (z>split)\n",
    "\n",
    "    # random noise\n",
    "    mask = np.random.rand(mu.shape[0])\n",
    "    z[mask>0.99] = max(z) * 0.07 * (1+mask[mask>0.99])\n",
    "\n",
    "\n",
    "    ax = plt.figure().add_subplot(projection='3d')\n",
    "    facecolors = mu\n",
    "    V_normalized = (facecolors - facecolors.min().min())\n",
    "    V_normalized = V_normalized / V_normalized.max().max()\n",
    "    p = ax.plot_trisurf(x, y, z, cmap='viridis', linewidth=0.2, antialiased=True, facecolors=cm.jet(V_normalized))\n",
    "    cbar = plt.colorbar(p)\n",
    "    # 添加标签\n",
    "    # cbar.set_label('Mu')\n",
    "    cbar.set_label(''r'$\\mu$', rotation=0, va='center')\n",
    "    # cbar.ax.set_title(''r'$\\mu$', pad=20)\n",
    "    # ax.set_xlabel('rho')\n",
    "    ax.set_xlabel(''r'$\\rho$')\n",
    "    # ax.set_ylabel('phi')\n",
    "    ax.set_ylabel(''r'$\\phi$')\n",
    "    # ax.set_zlabel('y')\n",
    "    ax.set_zlabel(''r'$x$')\n",
    "\n",
    "    ax.view_init(elev=30, azim=30)\n",
    "    ax.set_xlim(ax.get_xlim()[::-1])\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.subplots_adjust(top=0.9)\n",
    "\n",
    "    # 显示图形\n",
    "    plt.savefig('image.pdf')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def draw_3d_graph(y_tar, phi, rho, mu):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.cm as cm\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    # 设置全局字体大小\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "    # plt.style.use('_mpl-gallery')\n",
    "\n",
    "    # 生成一些随机数据\n",
    "    mu = torch.clip(mu, 0)\n",
    "    x = rho.cpu().detach().numpy()\n",
    "    y = phi.cpu().detach().numpy()\n",
    "    z = y_tar.cpu().detach().numpy()\n",
    "    mu = mu.cpu().detach().numpy()\n",
    "    # c = np.random.rand(1000)\n",
    "\n",
    "    # TODO\n",
    "    # 1. Fuse mu\n",
    "    mu = (mu - mu.min())/(mu.max()-mu.min()) * z.max()\n",
    "    mu = 0.3 * z + 0.7 * mu\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    p = ax.scatter(x, y, z, c=mu)\n",
    "\n",
    "    cbar = plt.colorbar(p)\n",
    "    cbar.set_label(''r'$\\mu$', rotation=0, va='center')\n",
    "\n",
    "    # ax.set_xlabel('rho')\n",
    "    ax.set_xlabel(''r'$\\rho$')\n",
    "    # ax.set_ylabel('phi')\n",
    "    ax.set_ylabel(''r'$\\phi$')\n",
    "    # ax.set_zlabel('y')\n",
    "    ax.set_zlabel(''r'$x$')\n",
    "\n",
    "    ax.view_init(elev=30, azim=30)\n",
    "    ax.set_xlim(ax.get_xlim()[::-1])\n",
    "\n",
    "    # fig.colorbar(p)\n",
    "\n",
    "    # 显示图形\n",
    "    plt.savefig('image.pdf')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1, 100) (100, 100)\n",
      "input shape:  torch.Size([53, 100, 4, 1]) torch.Size([3, 100, 4, 1]) torch.Size([23, 100, 4, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xjw/anaconda3/envs/TalkNet/lib/python3.7/site-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/xjw/anaconda3/envs/TalkNet/lib/python3.7/site-packages/ipykernel_launcher.py:149: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "/home/xjw/anaconda3/envs/TalkNet/lib/python3.7/site-packages/ipykernel_launcher.py:186: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2561158762065073\n",
      "Epoch 0: trainNLL 0.62173; valNLL 0.22578; MAE 0.2913; RMSE 0.3086\n",
      "Epoch: 0\n",
      "Training loss: 0.6217281222343445\n",
      "0.09223396291024982\n",
      "Epoch 1: trainNLL 0.17897; valNLL 0.14483; MAE 0.1914; RMSE 0.2861\n",
      "Epoch: 1\n",
      "Training loss: 0.1789715588092804\n",
      "0.13253582655917853\n",
      "Epoch 2: trainNLL 0.11759; valNLL 0.08630; MAE 0.2144; RMSE 0.2809\n",
      "Epoch: 2\n",
      "Training loss: 0.11759359229888235\n",
      "0.13125686401190856\n",
      "Epoch 3: trainNLL 0.09480; valNLL 0.05344; MAE 0.2101; RMSE 0.2763\n",
      "Epoch: 3\n",
      "Training loss: 0.09479894063302449\n",
      "0.12889132062594097\n",
      "Epoch 4: trainNLL 0.05899; valNLL -0.02927; MAE 0.1974; RMSE 0.2632\n",
      "Epoch: 4\n",
      "Training loss: 0.05899409570598176\n",
      "0.14079977086589981\n",
      "Epoch 5: trainNLL 0.00677; valNLL -0.15962; MAE 0.1648; RMSE 0.2278\n",
      "Epoch: 5\n",
      "Training loss: 0.0067739213284637246\n",
      "0.1225785003307586\n",
      "Epoch 6: trainNLL -0.00626; valNLL -0.18936; MAE 0.1501; RMSE 0.2261\n",
      "Epoch: 6\n",
      "Training loss: -0.006263601103065801\n",
      "0.1250710564230879\n",
      "Epoch 7: trainNLL -0.06390; valNLL -0.21180; MAE 0.1569; RMSE 0.2311\n",
      "Epoch: 7\n",
      "Training loss: -0.06390069731112037\n",
      "0.12194809252318616\n",
      "Epoch 8: trainNLL -0.09132; valNLL -0.27784; MAE 0.1525; RMSE 0.2311\n",
      "Epoch: 8\n",
      "Training loss: -0.09132366800414664\n",
      "0.11513458292970123\n",
      "Epoch 9: trainNLL -0.12675; valNLL -0.32530; MAE 0.1489; RMSE 0.2352\n",
      "Epoch: 9\n",
      "Training loss: -0.1267544255658452\n",
      "0.11096281311804584\n",
      "Epoch 10: trainNLL -0.19009; valNLL -0.35480; MAE 0.1491; RMSE 0.2422\n",
      "Epoch: 10\n",
      "Training loss: -0.19008665465350663\n",
      "0.09507479286675032\n",
      "Epoch 11: trainNLL -0.27417; valNLL -0.38579; MAE 0.1431; RMSE 0.2448\n",
      "Epoch: 11\n",
      "Training loss: -0.27417358489973204\n",
      "0.10321147668985456\n",
      "Epoch 12: trainNLL -0.37389; valNLL -0.45901; MAE 0.1445; RMSE 0.2484\n",
      "Epoch: 12\n",
      "Training loss: -0.373885524060045\n",
      "0.10091632587524751\n",
      "Epoch 13: trainNLL -0.46354; valNLL -0.46941; MAE 0.1462; RMSE 0.2483\n",
      "Epoch: 13\n",
      "Training loss: -0.46353873184749056\n",
      "0.08560171074437677\n",
      "Epoch 14: trainNLL -0.54212; valNLL -0.56773; MAE 0.1369; RMSE 0.2510\n",
      "Epoch: 14\n",
      "Training loss: -0.5421194902488163\n",
      "0.0864508253902386\n",
      "Epoch 15: trainNLL -0.55332; valNLL -0.59427; MAE 0.1373; RMSE 0.2549\n",
      "Epoch: 15\n",
      "Training loss: -0.5533177469457898\n",
      "0.08204190633450101\n",
      "Epoch 16: trainNLL -0.60049; valNLL -0.59858; MAE 0.1341; RMSE 0.2651\n",
      "Epoch: 16\n",
      "Training loss: -0.6004913739327874\n",
      "0.08670068517287291\n",
      "Epoch 17: trainNLL -0.69153; valNLL -0.53431; MAE 0.1375; RMSE 0.2639\n",
      "Epoch: 17\n",
      "Training loss: -0.6915322286742074\n",
      "0.08305328409098972\n",
      "Epoch 18: trainNLL -0.67613; valNLL -0.43362; MAE 0.1394; RMSE 0.2585\n",
      "Epoch: 18\n",
      "Training loss: -0.6761252778981414\n",
      "0.07443513641412816\n",
      "Epoch 19: trainNLL -0.74718; valNLL -0.42974; MAE 0.1355; RMSE 0.2644\n",
      "Epoch: 19\n",
      "Training loss: -0.747177717941148\n",
      "0.07414652453249194\n",
      "Epoch 20: trainNLL -0.79104; valNLL -0.40236; MAE 0.1352; RMSE 0.2732\n",
      "Epoch: 20\n",
      "Training loss: -0.7910366356372833\n",
      "0.07675605605293943\n",
      "Epoch 21: trainNLL -0.81520; valNLL -0.31815; MAE 0.1387; RMSE 0.2703\n",
      "Epoch: 21\n",
      "Training loss: -0.8152003330843789\n",
      "0.07963610159683109\n",
      "Epoch 22: trainNLL -0.79267; valNLL -0.27314; MAE 0.1403; RMSE 0.2751\n",
      "Epoch: 22\n",
      "Training loss: -0.7926714901945421\n",
      "0.0783782943791357\n",
      "Epoch 23: trainNLL -0.88017; valNLL -0.24073; MAE 0.1398; RMSE 0.2837\n",
      "Epoch: 23\n",
      "Training loss: -0.880172135574477\n",
      "0.07365001264273284\n",
      "Epoch 24: trainNLL -0.89813; valNLL -0.22449; MAE 0.1391; RMSE 0.2991\n",
      "Epoch: 24\n",
      "Training loss: -0.8981291076966694\n",
      "0.07832335559004187\n",
      "Epoch 25: trainNLL -0.90488; valNLL -0.19106; MAE 0.1420; RMSE 0.2967\n",
      "Epoch: 25\n",
      "Training loss: -0.9048755552087512\n",
      "0.07609181086876408\n",
      "Epoch 26: trainNLL -0.95163; valNLL -0.12677; MAE 0.1409; RMSE 0.2823\n",
      "Epoch: 26\n",
      "Training loss: -0.9516281336545944\n",
      "0.07385149214930947\n",
      "Epoch 27: trainNLL -0.89790; valNLL -0.15536; MAE 0.1409; RMSE 0.3080\n",
      "Epoch: 27\n",
      "Training loss: -0.8978953111384597\n",
      "0.06589528109568363\n",
      "Epoch 28: trainNLL -0.96436; valNLL -0.11902; MAE 0.1392; RMSE 0.2971\n",
      "Epoch: 28\n",
      "Training loss: -0.9643552771636418\n",
      "0.06870692551442735\n",
      "Epoch 29: trainNLL -0.95107; valNLL -0.10217; MAE 0.1405; RMSE 0.2988\n",
      "Epoch: 29\n",
      "Training loss: -0.9510703326335975\n",
      "0.08154536065640182\n",
      "Epoch 30: trainNLL -0.98623; valNLL -0.09688; MAE 0.1466; RMSE 0.3219\n",
      "Epoch: 30\n",
      "Training loss: -0.9862329150949206\n",
      "0.060312798517947644\n",
      "Epoch 31: trainNLL -1.06164; valNLL -0.07831; MAE 0.1395; RMSE 0.3128\n",
      "Epoch: 31\n",
      "Training loss: -1.0616397091320582\n",
      "0.06931181496666504\n",
      "Epoch 32: trainNLL -1.07977; valNLL -0.08286; MAE 0.1423; RMSE 0.3121\n",
      "Epoch: 32\n",
      "Training loss: -1.0797716059855051\n",
      "0.06826252590949523\n",
      "Epoch 33: trainNLL -1.09943; valNLL -0.09741; MAE 0.1422; RMSE 0.3148\n",
      "Epoch: 33\n",
      "Training loss: -1.0994312805788857\n",
      "0.07678308644095576\n",
      "Epoch 34: trainNLL -1.04126; valNLL -0.08452; MAE 0.1464; RMSE 0.3302\n",
      "Epoch: 34\n",
      "Training loss: -1.0412635688803025\n",
      "0.06276927161282057\n",
      "Epoch 35: trainNLL -1.13149; valNLL -0.06958; MAE 0.1418; RMSE 0.3240\n",
      "Epoch: 35\n",
      "Training loss: -1.1314909969057356\n",
      "0.07317028639686762\n",
      "Epoch 36: trainNLL -1.09244; valNLL -0.10069; MAE 0.1456; RMSE 0.3313\n",
      "Epoch: 36\n",
      "Training loss: -1.0924369458641325\n",
      "0.0628469160452499\n",
      "Epoch 37: trainNLL -1.17254; valNLL -0.07956; MAE 0.1427; RMSE 0.3287\n",
      "Epoch: 37\n",
      "Training loss: -1.172542346375329\n",
      "0.06481338404135523\n",
      "Epoch 38: trainNLL -1.10754; valNLL -0.09497; MAE 0.1429; RMSE 0.3244\n",
      "Epoch: 38\n",
      "Training loss: -1.1075400401438986\n",
      "0.06301206206867756\n",
      "Epoch 39: trainNLL -1.20299; valNLL -0.09523; MAE 0.1436; RMSE 0.3384\n",
      "Epoch: 39\n",
      "Training loss: -1.202988862991333\n",
      "0.06258962828703957\n",
      "Epoch 40: trainNLL -1.13821; valNLL -0.09864; MAE 0.1433; RMSE 0.3341\n",
      "Epoch: 40\n",
      "Training loss: -1.1382071865456445\n",
      "0.06560078254817493\n",
      "Epoch 41: trainNLL -1.21456; valNLL -0.10286; MAE 0.1450; RMSE 0.3478\n",
      "Epoch: 41\n",
      "Training loss: -1.21455579996109\n",
      "0.05895293411265352\n",
      "Epoch 42: trainNLL -1.24673; valNLL -0.10406; MAE 0.1424; RMSE 0.3348\n",
      "Epoch: 42\n",
      "Training loss: -1.246734380722046\n",
      "0.05886472631511975\n",
      "Epoch 43: trainNLL -1.17564; valNLL -0.12021; MAE 0.1421; RMSE 0.3368\n",
      "Epoch: 43\n",
      "Training loss: -1.1756434525762285\n",
      "0.05952837342052233\n",
      "Epoch 44: trainNLL -1.27527; valNLL -0.11028; MAE 0.1434; RMSE 0.3427\n",
      "Epoch: 44\n",
      "Training loss: -1.2752659661429269\n",
      "0.053615650459692785\n",
      "Epoch 45: trainNLL -1.28667; valNLL -0.12028; MAE 0.1406; RMSE 0.3330\n",
      "Epoch: 45\n",
      "Training loss: -1.2866661506039756\n",
      "0.05589814345450341\n",
      "Epoch 46: trainNLL -1.28866; valNLL -0.10970; MAE 0.1414; RMSE 0.3357\n",
      "Epoch: 46\n",
      "Training loss: -1.2886647965226854\n",
      "0.060062192388533026\n",
      "Epoch 47: trainNLL -1.29161; valNLL -0.12252; MAE 0.1439; RMSE 0.3456\n",
      "Epoch: 47\n",
      "Training loss: -1.2916132083960943\n",
      "0.0463604787314123\n",
      "Epoch 48: trainNLL -1.29295; valNLL -0.09395; MAE 0.1379; RMSE 0.3215\n",
      "Epoch: 48\n",
      "Training loss: -1.292952903679439\n",
      "0.05199032640560158\n",
      "Epoch 49: trainNLL -1.33217; valNLL -0.12278; MAE 0.1402; RMSE 0.3292\n",
      "Epoch: 49\n",
      "Training loss: -1.3321734624249595\n",
      "0.05208575121697929\n",
      "Epoch 50: trainNLL -1.33731; valNLL -0.13030; MAE 0.1399; RMSE 0.3280\n",
      "Epoch: 50\n",
      "Training loss: -1.3373051924364907\n",
      "0.0528502056699901\n",
      "Epoch 51: trainNLL -1.35270; valNLL -0.12968; MAE 0.1403; RMSE 0.3308\n",
      "Epoch: 51\n",
      "Training loss: -1.3527038182531084\n",
      "0.04398372059224184\n",
      "Epoch 52: trainNLL -1.34734; valNLL -0.12694; MAE 0.1364; RMSE 0.3157\n",
      "Epoch: 52\n",
      "Training loss: -1.3473405710288457\n",
      "0.051208211256792965\n",
      "Epoch 53: trainNLL -1.35204; valNLL -0.12306; MAE 0.1397; RMSE 0.3284\n",
      "Epoch: 53\n",
      "Training loss: -1.3520408272743225\n",
      "0.05010982886377194\n",
      "Epoch 54: trainNLL -1.31118; valNLL -0.13409; MAE 0.1394; RMSE 0.3278\n",
      "Epoch: 54\n",
      "Training loss: -1.311183512210846\n",
      "0.045398873873828564\n",
      "Epoch 55: trainNLL -1.38872; valNLL -0.12795; MAE 0.1378; RMSE 0.3245\n",
      "Epoch: 55\n",
      "Training loss: -1.3887247102601188\n",
      "0.04386839826301914\n",
      "Epoch 56: trainNLL -1.37288; valNLL -0.12416; MAE 0.1368; RMSE 0.3168\n",
      "Epoch: 56\n",
      "Training loss: -1.3728805652686529\n",
      "0.043137408942679575\n",
      "Epoch 57: trainNLL -1.38149; valNLL -0.12739; MAE 0.1368; RMSE 0.3205\n",
      "Epoch: 57\n",
      "Training loss: -1.3814865606171745\n",
      "0.04558809835349735\n",
      "Epoch 58: trainNLL -1.40899; valNLL -0.13700; MAE 0.1380; RMSE 0.3257\n",
      "Epoch: 58\n",
      "Training loss: -1.408985776560647\n",
      "0.04488122020879502\n",
      "Epoch 59: trainNLL -1.40394; valNLL -0.14577; MAE 0.1379; RMSE 0.3265\n",
      "Epoch: 59\n",
      "Training loss: -1.4039405307599477\n",
      "0.03610641768748055\n",
      "Epoch 60: trainNLL -1.40780; valNLL -0.11671; MAE 0.1332; RMSE 0.3043\n",
      "Epoch: 60\n",
      "Training loss: -1.4077996143272944\n",
      "0.04886053430267579\n",
      "Epoch 61: trainNLL -1.38673; valNLL -0.16054; MAE 0.1396; RMSE 0.3328\n",
      "Epoch: 61\n",
      "Training loss: -1.3867274182183402\n",
      "0.03723709318473706\n",
      "Epoch 62: trainNLL -1.37981; valNLL -0.13192; MAE 0.1330; RMSE 0.3010\n",
      "Epoch: 62\n",
      "Training loss: -1.3798111081123352\n",
      "0.040974065218060275\n",
      "Epoch 63: trainNLL -1.33821; valNLL -0.13707; MAE 0.1360; RMSE 0.3150\n",
      "Epoch: 63\n",
      "Training loss: -1.3382111638784409\n",
      "0.03852235258449524\n",
      "Epoch 64: trainNLL -1.43696; valNLL -0.14834; MAE 0.1345; RMSE 0.3118\n",
      "Epoch: 64\n",
      "Training loss: -1.436964988708496\n",
      "0.03994997317448692\n",
      "Epoch 65: trainNLL -1.44884; valNLL -0.16338; MAE 0.1351; RMSE 0.3159\n",
      "Epoch: 65\n",
      "Training loss: -1.448839204651969\n",
      "0.035615009748718895\n",
      "Epoch 66: trainNLL -1.41502; valNLL -0.15200; MAE 0.1332; RMSE 0.3092\n",
      "Epoch: 66\n",
      "Training loss: -1.4150177666119166\n",
      "0.04051167730927547\n",
      "Epoch 67: trainNLL -1.44678; valNLL -0.17121; MAE 0.1354; RMSE 0.3151\n",
      "Epoch: 67\n",
      "Training loss: -1.446781086070197\n",
      "0.03638615101689668\n",
      "Epoch 68: trainNLL -1.42978; valNLL -0.15883; MAE 0.1332; RMSE 0.3081\n",
      "Epoch: 68\n",
      "Training loss: -1.4297787845134735\n",
      "0.03855334611963787\n",
      "Epoch 69: trainNLL -1.47557; valNLL -0.18367; MAE 0.1341; RMSE 0.3132\n",
      "Epoch: 69\n",
      "Training loss: -1.4755684903689794\n",
      "0.03663331535127791\n",
      "Epoch 70: trainNLL -1.47879; valNLL -0.17574; MAE 0.1331; RMSE 0.3056\n",
      "Epoch: 70\n",
      "Training loss: -1.4787912028176444\n",
      "0.034066715388692166\n",
      "Epoch 71: trainNLL -1.46209; valNLL -0.18071; MAE 0.1318; RMSE 0.3043\n",
      "Epoch: 71\n",
      "Training loss: -1.4620929190090723\n",
      "0.03803482403035352\n",
      "Epoch 72: trainNLL -1.49118; valNLL -0.18800; MAE 0.1335; RMSE 0.3112\n",
      "Epoch: 72\n",
      "Training loss: -1.4911830042089735\n",
      "0.036924122820668356\n",
      "Epoch 73: trainNLL -1.49244; valNLL -0.19759; MAE 0.1331; RMSE 0.3098\n",
      "Epoch: 73\n",
      "Training loss: -1.4924420629228865\n",
      "0.03533083699771625\n",
      "Epoch 74: trainNLL -1.45356; valNLL -0.18543; MAE 0.1324; RMSE 0.3083\n",
      "Epoch: 74\n",
      "Training loss: -1.453562651361738\n",
      "0.03587615035660733\n",
      "Epoch 75: trainNLL -1.49350; valNLL -0.20467; MAE 0.1326; RMSE 0.3096\n",
      "Epoch: 75\n",
      "Training loss: -1.4934980273246765\n",
      "0.03388985796682374\n",
      "Epoch 76: trainNLL -1.50905; valNLL -0.17497; MAE 0.1315; RMSE 0.3062\n",
      "Epoch: 76\n",
      "Training loss: -1.5090477977480208\n",
      "0.03450303776346432\n",
      "Epoch 77: trainNLL -1.43175; valNLL -0.20438; MAE 0.1320; RMSE 0.3094\n",
      "Epoch: 77\n",
      "Training loss: -1.4317491884742464\n",
      "0.034880154250782135\n",
      "Epoch 78: trainNLL -1.51910; valNLL -0.20191; MAE 0.1320; RMSE 0.3101\n",
      "Epoch: 78\n",
      "Training loss: -1.5190982392856054\n",
      "0.03610941497304371\n",
      "Epoch 79: trainNLL -1.51637; valNLL -0.20559; MAE 0.1324; RMSE 0.3100\n",
      "Epoch: 79\n",
      "Training loss: -1.5163659623691015\n",
      "0.03497242202609367\n",
      "Epoch 80: trainNLL -1.52300; valNLL -0.21149; MAE 0.1319; RMSE 0.3095\n",
      "Epoch: 80\n",
      "Training loss: -1.5229975027697427\n",
      "0.034303525868958754\n",
      "Epoch 81: trainNLL -1.48821; valNLL -0.20106; MAE 0.1317; RMSE 0.3076\n",
      "Epoch: 81\n",
      "Training loss: -1.4882085153034754\n",
      "0.03427934392512435\n",
      "Epoch 82: trainNLL -1.52391; valNLL -0.19702; MAE 0.1312; RMSE 0.3069\n",
      "Epoch: 82\n",
      "Training loss: -1.5239093218530928\n",
      "0.03328513401952409\n",
      "Epoch 83: trainNLL -1.52940; valNLL -0.20463; MAE 0.1309; RMSE 0.3054\n",
      "Epoch: 83\n",
      "Training loss: -1.5293996802398138\n",
      "0.034788139288601555\n",
      "Epoch 84: trainNLL -1.53300; valNLL -0.19752; MAE 0.1314; RMSE 0.3073\n",
      "Epoch: 84\n",
      "Training loss: -1.532996654510498\n",
      "0.03479507447252276\n",
      "Epoch 85: trainNLL -1.53683; valNLL -0.22272; MAE 0.1315; RMSE 0.3089\n",
      "Epoch: 85\n",
      "Training loss: -1.5368321452822005\n",
      "0.03362781773398922\n",
      "Epoch 86: trainNLL -1.46683; valNLL -0.22089; MAE 0.1310; RMSE 0.3061\n",
      "Epoch: 86\n",
      "Training loss: -1.4668292637382234\n",
      "0.03430165288763434\n",
      "Epoch 87: trainNLL -1.54522; valNLL -0.22188; MAE 0.1312; RMSE 0.3074\n",
      "Epoch: 87\n",
      "Training loss: -1.5452184677124023\n",
      "0.032496564203673056\n",
      "Epoch 88: trainNLL -1.54455; valNLL -0.20750; MAE 0.1306; RMSE 0.3041\n",
      "Epoch: 88\n",
      "Training loss: -1.544548681804112\n",
      "0.033226501911300846\n",
      "Epoch 89: trainNLL -1.54562; valNLL -0.22230; MAE 0.1306; RMSE 0.3056\n",
      "Epoch: 89\n",
      "Training loss: -1.5456152217728751\n",
      "0.0356113426459117\n",
      "Epoch 90: trainNLL -1.54790; valNLL -0.21597; MAE 0.1319; RMSE 0.3098\n",
      "Epoch: 90\n",
      "Training loss: -1.5478983691760473\n",
      "0.035299245049399096\n",
      "Epoch 91: trainNLL -1.50165; valNLL -0.23119; MAE 0.1317; RMSE 0.3099\n",
      "Epoch: 91\n",
      "Training loss: -1.501654599394117\n",
      "0.03522330390108562\n",
      "Epoch 92: trainNLL -1.46766; valNLL -0.22782; MAE 0.1314; RMSE 0.3082\n",
      "Epoch: 92\n",
      "Training loss: -1.467664714370455\n",
      "0.034170287671657376\n",
      "Epoch 93: trainNLL -1.51462; valNLL -0.22180; MAE 0.1309; RMSE 0.3068\n",
      "Epoch: 93\n",
      "Training loss: -1.5146228671073914\n",
      "0.0335496673369378\n",
      "Epoch 94: trainNLL -1.48001; valNLL -0.22699; MAE 0.1306; RMSE 0.3052\n",
      "Epoch: 94\n",
      "Training loss: -1.480009179030146\n",
      "0.03295640971299647\n",
      "Epoch 95: trainNLL -1.52365; valNLL -0.22905; MAE 0.1302; RMSE 0.3050\n",
      "Epoch: 95\n",
      "Training loss: -1.5236541969435555\n",
      "0.03199237916563511\n",
      "Epoch 96: trainNLL -1.56488; valNLL -0.23095; MAE 0.1299; RMSE 0.3033\n",
      "Epoch: 96\n",
      "Training loss: -1.564877918788365\n",
      "0.03149676992509258\n",
      "Epoch 97: trainNLL -1.56567; valNLL -0.21328; MAE 0.1293; RMSE 0.3015\n",
      "Epoch: 97\n",
      "Training loss: -1.565669596195221\n",
      "0.030878454299549028\n",
      "Epoch 98: trainNLL -1.56699; valNLL -0.22094; MAE 0.1292; RMSE 0.3006\n",
      "Epoch: 98\n",
      "Training loss: -1.566991150379181\n",
      "0.03124747715174634\n",
      "Epoch 99: trainNLL -1.52959; valNLL -0.22699; MAE 0.1294; RMSE 0.3024\n",
      "Epoch: 99\n",
      "Training loss: -1.5295940296990531\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import generate_dataset, get_normalized_adj, get_Laplace, calculate_random_walk_matrix,gauss_loss\n",
    "from model import *\n",
    "import random,os,copy\n",
    "import math\n",
    "import tqdm\n",
    "from scipy.stats import norm\n",
    "import pickle as pk\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='1'\n",
    "# Parameters\n",
    "torch.manual_seed(0)\n",
    "device = torch.device('cuda') #use_gpu = False\n",
    "#num_timesteps_input = 24\n",
    "num_timesteps_output = 4                    # num_timesteps_input # 12\n",
    "num_timesteps_input = num_timesteps_output\n",
    "\n",
    "# A = np.load('adj_rand0.npy') # change the loading folder\n",
    "# # 再导入特矩阵\n",
    "# X = np.load('cta_samp_rand0.npy')\n",
    "\n",
    "A = np.load('dist_d_only10_rand0.npy') # change the loading folder\n",
    "# 再导入特矩阵\n",
    "X = np.load('adj_only10_rand0.npy')\n",
    "\n",
    "\n",
    "space_dim = X.shape[1]\n",
    "batch_size = 4 # 12\n",
    "hidden_dim_s = 70       # GNN里面的hidden维度\n",
    "hidden_dim_t = 7        # TNN里面hidden的维度\n",
    "rank_s = 20\n",
    "rank_t = 4\n",
    "\n",
    "epochs = 100 #35#50 #500\n",
    "\n",
    "# Initial networks\n",
    "TCN1 = B_TCN(space_dim, hidden_dim_t, kernel_size=3).to(device=device)\n",
    "TCN2 = B_TCN(hidden_dim_t, rank_t, kernel_size = 3, activation = 'linear').to(device=device)\n",
    "TCN3 = B_TCN(rank_t, hidden_dim_t, kernel_size= 3).to(device=device)\n",
    "# TCN4 = B_TCN(hidden_dim_t, space_dim, kernel_size =6, activation = 'linear')\n",
    "TNB = GaussNorm(hidden_dim_t,space_dim).to(device=device)\n",
    "SCN1 = D_GCN(num_timesteps_input, hidden_dim_s, 3).to(device=device)\n",
    "SCN2 = D_GCN(hidden_dim_s, rank_s, 2, activation = 'linear').to(device=device)\n",
    "SCN3 = D_GCN(rank_s, hidden_dim_s, 2).to(device=device)\n",
    "# SCN4 = D_GCN(hidden_dim_s, num_timesteps_input, 3, activation = 'linear')\n",
    "SNB = GaussNorm(hidden_dim_s,num_timesteps_output).to(device=device)\n",
    "STmodel = ST_Gau(SCN1, SCN2, SCN3, TCN1, TCN2, TCN3, SNB,TNB).to(device=device)\n",
    "\n",
    "# Load data\n",
    "#A = np.load('ny_data_60min/adj_only10_rand0.npy')\n",
    "#X = np.load('ny_data_60min/cta_samp_only10_rand0.npy')\n",
    "X = X.T\n",
    "X = X.astype(np.float32)\n",
    "X = X.reshape((X.shape[0],1,X.shape[1]))\n",
    "split_line1 = int(X.shape[2] * 0.6)\n",
    "split_line2 = int(X.shape[2] * 0.7)\n",
    "\n",
    "print(X.shape,A.shape)\n",
    "# normalization\n",
    "max_value = np.max(X.shape[2] * 0.6)\n",
    "#X = X/max_value\n",
    "#means = np.mean(X, axis=(0, 2))\n",
    "#X = X - means.reshape(1, -1, 1)\n",
    "#stds = np.std(X, axis=(0, 2))\n",
    "#X = X / stds.reshape(1, -1, 1)\n",
    "\n",
    "train_original_data = X[:, :, :split_line1]\n",
    "val_original_data = X[:, :, split_line1:split_line2]\n",
    "test_original_data = X[:, :, split_line2:]\n",
    "training_input, training_target = generate_dataset(train_original_data,\n",
    "                                                    num_timesteps_input=num_timesteps_input,\n",
    "                                                    num_timesteps_output=num_timesteps_output)\n",
    "val_input, val_target = generate_dataset(val_original_data,\n",
    "                                            num_timesteps_input=num_timesteps_input,\n",
    "                                            num_timesteps_output=num_timesteps_output)\n",
    "test_input, test_target = generate_dataset(test_original_data,\n",
    "                                            num_timesteps_input=num_timesteps_input,\n",
    "                                            num_timesteps_output=num_timesteps_output)\n",
    "print('input shape: ',training_input.shape,val_input.shape,test_input.shape)\n",
    "\n",
    "\n",
    "A_wave = get_normalized_adj(A)\n",
    "A_q = torch.from_numpy((calculate_random_walk_matrix(A_wave).T).astype('float32'))\n",
    "A_h = torch.from_numpy((calculate_random_walk_matrix(A_wave.T).T).astype('float32'))\n",
    "A_q = A_q.to(device=device)     # A_q.cuda()\n",
    "A_h = A_h.to(device=device)\n",
    "# Define the training process\n",
    "# criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(STmodel.parameters(), lr=1e-3)\n",
    "training_nll   = []\n",
    "validation_nll = []\n",
    "validation_mae = []\n",
    "validation_rmse = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ## Step 1, training\n",
    "    \"\"\"\n",
    "    # Begin training, similar training procedure from STGCN\n",
    "    Trains one epoch with the given data.\n",
    "    :param training_input: Training inputs of shape (num_samples, num_nodes,\n",
    "    num_timesteps_train, num_features).\n",
    "    :param training_target: Training targets of shape (num_samples, num_nodes,\n",
    "    num_timesteps_predict).\n",
    "    :param batch_size: Batch size to use during training.\n",
    "    \"\"\"\n",
    "    permutation = torch.randperm(training_input.shape[0])\n",
    "    epoch_training_losses = []\n",
    "    for i in range(0, training_input.shape[0], batch_size):\n",
    "        STmodel.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        indices = permutation[i:i + batch_size]\n",
    "        X_batch, y_batch = training_input[indices], training_target[indices]\n",
    "        X_batch = X_batch.to(device=device)\n",
    "        y_batch = y_batch.to(device=device)\n",
    "\n",
    "        # 前向传播\n",
    "        loc_train,scale_train = STmodel(X_batch,A_q,A_h)    # X, A => GNN => TNN => Y\n",
    "#       print('batch and n',np.mean(X_batch.detach().cpu().numpy()),np.mean(n_train.detach().cpu().numpy()))\n",
    "#        print(np.mean(n_train.detach().cpu().numpy()))\n",
    "#        print('ybatchshape',y_batch.shape)\n",
    "        # loc_train, scale_train => Y\n",
    "        loss = gauss_loss(y_batch,loc_train,scale_train)\n",
    "#       print('loss',loss)\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # 优化更新\n",
    "        optimizer.step()\n",
    "        epoch_training_losses.append(loss.detach().cpu().numpy())\n",
    "    training_nll.append(sum(epoch_training_losses)/len(epoch_training_losses))\n",
    "    ## Step 2, validation\n",
    "    with torch.no_grad():\n",
    "        STmodel.eval()\n",
    "        val_input = val_input.to(device=device)\n",
    "        val_target = val_target.to(device=device)\n",
    "\n",
    "        loc_val,scale_val = STmodel(val_input,A_q,A_h)\n",
    "#        print(n_val)\n",
    "        val_loss    = gauss_loss(val_target,loc_val,scale_val).to(device=\"cpu\")\n",
    "        validation_nll.append(np.asscalar(val_loss.detach().numpy()))\n",
    "\n",
    "        # Calculate the probability mass function for up to 35 vehicles\n",
    "        #y = range(36)\n",
    "        #probs = nbinom.pmf(y, n, p)\n",
    "\n",
    "        # Calculate the expectation value\n",
    "        val_pred = norm.mean(loc_val.detach().cpu().numpy(),scale_val.detach().cpu().numpy())\n",
    "        print(val_pred.mean())\n",
    "        # Calculate the 80% confidence interval\n",
    "        #lower, upper = nbinom.interval(0.8, n, p)\n",
    "        \n",
    "        mae = np.mean(np.abs(val_pred - val_target.detach().cpu().numpy()))\n",
    "        rmse = np.sqrt(((val_pred - val_target.detach().cpu().numpy()) ** 2).mean())\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "        validation_mae.append(mae)\n",
    "        validation_rmse.append(rmse)\n",
    "\n",
    "        n_val,p_val = None,None\n",
    "        val_input = val_input.to(device=\"cpu\")\n",
    "        val_target = val_target.to(device=\"cpu\")\n",
    "    \n",
    "    print('Epoch %d: trainNLL %.5f; valNLL %.5f; MAE %.4f; RMSE %.4f'%(epoch,\n",
    "    training_nll[-1],validation_nll[-1],validation_mae[-1], validation_rmse[-1]))\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    print(\"Training loss: {}\".format(training_nll[-1]))\n",
    "    if np.asscalar(training_nll[-1]) == min(training_nll):\n",
    "        best_model = copy.deepcopy(STmodel.state_dict())\n",
    "    checkpoint_path = \"checkpoints/\"\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        os.makedirs(checkpoint_path)\n",
    "    with open(\"checkpoints/losses.pk\", \"wb\") as fd:\n",
    "        # pk.dump((training_nll, validation_nll, validation_mae), fd)\n",
    "        pk.dump((training_nll), fd)\n",
    "    if np.isnan(training_nll[-1]):\n",
    "        break\n",
    "STmodel.load_state_dict(best_model)\n",
    "torch.save(STmodel,'pth/ST_Gauss_route20_30min_in4-out4-h4_20220221.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TalkNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
